{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/ConvexICP/data_convex.py:46: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  f = h5py.File(h5_name)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from data import ModelNet40,download,load_data\n",
    "\n",
    "# from data import ModelNet40Convex,download,load_data\n",
    "from data_convex import ModelNet40Convex,download,load_data\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from geomloss import SamplesLoss\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import warnings\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torch import jit\n",
    "%matplotlib inline\n",
    "import os\n",
    "from torch.utils.data import Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install geomloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_results_exp = []\n",
    "test_accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 2 every 30 epochs\"\"\"\n",
    "    lrt = lr * (0.5 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/ConvexICP/data_convex.py:46: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  f = h5py.File(h5_name)\n",
      "/workspace/ConvexICP/data.py:36: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  f = h5py.File(h5_name)\n"
     ]
    }
   ],
   "source": [
    "data = ModelNet40Convex(num_points=2048, partition='train', gaussian_noise=False,\n",
    "                       unseen=False, factor=4)\n",
    "data_test = ModelNet40(num_points=2048, partition='test', gaussian_noise=False,\n",
    "                       unseen=False, factor=4)\n",
    "train_loader = DataLoader(data, batch_size=32, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(data_test, batch_size=1, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Original\n",
    "# data = ModelNet40(num_points=2048, partition='train', gaussian_noise=False,\n",
    "#                        unseen=False, factor=4)\n",
    "# data_test = ModelNet40(num_points=2048, partition='test', gaussian_noise=False,\n",
    "#                        unseen=False, factor=4)\n",
    "# train_loader = DataLoader(data, batch_size=128, shuffle=True, drop_last=True)\n",
    "# test_loader = DataLoader(data_test, batch_size=1, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loader\n",
      "0 (3, 2048)\n",
      "1 (3, 2048)\n",
      "2 (3, 2048)\n",
      "3 (3, 2048)\n",
      "4 2\n",
      "5 2\n",
      "6 2\n",
      "7 2\n"
     ]
    }
   ],
   "source": [
    "print('Test Loader')\n",
    "sample=data[0]\n",
    "\n",
    "len(sample)\n",
    "for i,e in enumerate(sample):\n",
    "    try:\n",
    "        print(i,e.shape)\n",
    "    except:\n",
    "        print(i,len(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_icp(model,device, train_loader,test_loader, optimizer,criterion, epoch, eval_mode='on'):\n",
    "    global best\n",
    "    model.train()\n",
    "    final_cal = nn.Sigmoid()\n",
    "    train_error_logs = []\n",
    "    t1 = datetime.now()\n",
    "    test_loss_log = []\n",
    "    test_acc_log = []\n",
    "    train_corrects = 0\n",
    "    \n",
    "    for batch_idx, (src, target, rotation_ab, translation_ab, rotation_ba, translation_ba, euler_ab, euler_ba) in enumerate(train_loader):\n",
    "        src = src.to(device)\n",
    "        target = target.to(device)\n",
    "        rotation_ab = rotation_ab.to(device)\n",
    "        translation_ab = translation_ab.to(device)\n",
    "        rotation_ba = rotation_ba.to(device)\n",
    "        translation_ba = translation_ba.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output_train = model(src,target,rotation_ab,translation_ab)\n",
    "        loss = criterion(output_train, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #pred_train = output_train.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "        #train_corrects += pred_train.eq(target.view_as(pred_train)).sum().item()\n",
    "        train_error_logs.append(loss.item())\n",
    "        \n",
    "        if (batch_idx+0) % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime: {:.2f}'.format(\n",
    "                epoch, (batch_idx+0)* len(target), len(train_loader.dataset),\n",
    "                100. * (batch_idx+0) / len(train_loader), loss,(datetime.now()-t1).total_seconds()))\n",
    "\n",
    "        \n",
    "    print('Train Epoch: {} Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "                epoch, train_corrects, len(train_loader.dataset),\n",
    "                100. * train_corrects / len(train_loader.dataset)))\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, target, rotation_ab, translation_ab, rotation_ba, translation_ba, euler_ab, euler_ba in enumerate(test_loader):\n",
    "            src = src.to(device)\n",
    "            target = target.to(device)\n",
    "            rotation_ab = rotation_ab.to(device)\n",
    "            translation_ab = translation_ab.to(device)\n",
    "            rotation_ba = rotation_ba.to(device)\n",
    "            translation_ba = translation_ba.to(device)\n",
    "            output_test = model(src,target,rotation_ab,translation_ab)\n",
    "            test_loss += criterion(output_test, target_test).item()  # sum up batch loss\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_loss_log.append(test_loss)\n",
    "    print('Test set: Average loss: {:.8f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct_test, len(test_loader.dataset),\n",
    "            100. * correct_test / len(test_loader.dataset)))\n",
    "    return train_error_logs,test_loss_log,test_acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet(nn.Module):\n",
    "    def __init__(self, emb_dims=512):\n",
    "        super(PointNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(3, 64, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n",
    "        self.conv3 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n",
    "        self.conv4 = nn.Conv1d(64, 128, kernel_size=1, bias=False)\n",
    "        self.conv5 = nn.Conv1d(128, emb_dims, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.bn5 = nn.BatchNorm1d(emb_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPNet(nn.Module):\n",
    "    def __init__(self, emb_dims=512):\n",
    "        super(CPNet, self).__init__()\n",
    "        self.base_net = PointNet()\n",
    "        self.loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.05)\n",
    "\n",
    "        \n",
    "    def forward(self, x1, x2, R, T):\n",
    "        \n",
    "        trans = torch.matmul(R, x1) + T.unsqueeze(2)\n",
    "        emb1 = self.base_net(trans)\n",
    "        emb2 = self.base_net(x2)\n",
    "        \n",
    "        dist = self.loss(emb1,emb2)\n",
    "\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.5352e+22,  4.5769e-41,  9.8596e+02],\n",
       "         [ 3.0653e-41,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(1,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPEvalNet(nn.Module):\n",
    "    def __init__(self, base_net, emb_dims=512):\n",
    "        super(CPEvalNet, self).__init__()\n",
    "        self.base_net = PointNet()\n",
    "        self.base_net.load_state_dict(base_net.state_dict())\n",
    "        for param in self.base_net:\n",
    "            param.require_grad = False\n",
    "        self.rotation = nn.Parameter(torch.Tensor(1,3,3))\n",
    "        self.translation = nn.Parameter(torch.Tensor(1,3))\n",
    "        nn.init.kaiming_uniform_(self.rotation, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.translation, a=math.sqrt(5))\n",
    "        self.loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.05)\n",
    "\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        \n",
    "        trans = torch.matmul(self.rotation, x1) + self.translation.unsqueeze(2)\n",
    "        emb1 = self.base_net(trans)\n",
    "        emb2 = self.base_net(x2)\n",
    "        \n",
    "        dist = self.loss(emb1,emb2)\n",
    "\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_loss(output, target):\n",
    "    loss = torch.mean(output)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CPNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, content in enumerate(train_loader):\n",
    "    (src, pointcloud1, pointcloud2, pointcloud3,mat1,mat2,mat3_0,mat3_1)= content\n",
    "    print(batch_idx)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(src,pointcloud1,R_ab1,translation_ab1)\n",
    "\n",
    "# type(pointcloud1),type(R_ab1),type(translation_ab1),type(src),model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = dummy_loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "best=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CPNet(\n",
       "  (base_net): PointNet(\n",
       "    (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (conv2): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (conv3): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (conv4): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (conv5): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (loss): SamplesLoss()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(src.shape,pointcloud1.shape,R_ab1.shape,translation_ab1.shape)\n",
    "\n",
    "# trans = torch.matmul(R, x1) + T.unsqueeze(2)\n",
    "# trans = torch.matmul(R_ab1, src) + translation_ab1.unsqueeze(2)\n",
    "\n",
    "# translation_ab1.unsqueeze(2).shape\n",
    "# print(src.shape,target.shape,rotation_ab.shape, translation_ab.shape)\n",
    "# src\n",
    "# rotation_ab\n",
    "# .float()\n",
    "# trans = torch.matmul(R_ab1.float(), src.float()) + translation_ab1.float().unsqueeze(2)\n",
    "# trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global best\n",
    "# model.train()\n",
    "# final_cal = nn.Sigmoid()\n",
    "# train_error_logs = []\n",
    "# t1 = datetime.now()\n",
    "# test_loss_log = []\n",
    "# test_acc_log = []\n",
    "\n",
    "# for batch_idx, content in enumerate(train_loader):\n",
    "#     (src, pointcloud1, pointcloud2, pointcloud3,mat1,mat2,mat3_0,mat3_1)=content\n",
    "#     src=src.to(device)\n",
    "# #         target = target.to(device)\n",
    "#     pointcloud1=pointcloud1.to(device)\n",
    "#     pointcloud2=pointcloud2.to(device)\n",
    "#     pointcloud3=pointcloud3.to(device)\n",
    "\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     translation_ab1,R_ab1=mat1[0],mat1[1]\n",
    "\n",
    "#     translation_ab2,R_ab2=mat2[0],mat2[1]\n",
    "\n",
    "#     translation_ab3_0,R_ab3_0=mat3_0[0],mat3_0[1]\n",
    "#     translation_ab3_1,R_ab3_1=mat3_1[0],mat3_1[1]\n",
    "#     translation_ab3=translation_ab3_0+translation_ab3_1\n",
    "#     R_ab3=R_ab3_0.matmul(R_ab3_1)\n",
    "\n",
    "#     translation_ab1 = translation_ab1.to(device)\n",
    "#     translation_ab2 = translation_ab2.to(device)\n",
    "#     translation_ab3 = translation_ab3.to(device)\n",
    "#     R_ab1 = R_ab1.to(device)\n",
    "#     R_ab2 = R_ab2.to(device)\n",
    "#     R_ab3 = R_ab3.to(device)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     output_train1 = model(src.float(),pointcloud1.float(),R_ab1.float(),translation_ab1.float())    \n",
    "#     output_train2 = model(src.float(),pointcloud2.float(),R_ab2.float(),translation_ab2.float())\n",
    "#     output_train3 = model(src.float(),pointcloud3.float(),R_ab3.float(),translation_ab3.float())\n",
    "\n",
    "\n",
    "#     loss1 = criterion(output_train1, pointcloud1)\n",
    "#     loss2 = criterion(output_train2, pointcloud2)\n",
    "#     loss3 = criterion(output_train3, pointcloud3)\n",
    "\n",
    "#     loss1.backward()\n",
    "#     loss2.backward()\n",
    "#     loss3.backward()\n",
    "    \n",
    "    \n",
    "#     optimizer.step()\n",
    "#     #pred_train = output_train.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "#     #train_corrects += pred_train.eq(target.view_as(pred_train)).sum().item()\n",
    "# #         train_error_logs.append(loss.item())\n",
    "#     train_error_logs.append(loss1.item())\n",
    "#     train_error_logs.append(loss2.item())\n",
    "#     train_error_logs.append(loss3.item())\n",
    "\n",
    "#     if (batch_idx+0) % 20 == 0:\n",
    "#         print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime: {:.2f}'.format(\n",
    "#             epoch, (batch_idx+0)* len(target), len(train_loader.dataset),\n",
    "#             100. * (batch_idx+0) / len(train_loader), loss,(datetime.now()-t1).total_seconds()))\n",
    "\n",
    "\n",
    "# print('Train Epoch: {} Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "#             epoch, train_corrects, len(train_loader.dataset),\n",
    "#             100. * train_corrects / len(train_loader.dataset)))\n",
    "\n",
    "# model.eval()\n",
    "# test_loss = 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_eval_convexity(model,device, train_loader,test_loader, optimizer,criterion, epoch, eval_mode='on'):\n",
    "    global best\n",
    "    model.train()\n",
    "    final_cal = nn.Sigmoid()\n",
    "    train_error_logs = []\n",
    "    t1 = datetime.now()\n",
    "    test_loss_log = []\n",
    "    test_acc_log = []\n",
    "    train_corrects = 0\n",
    "    \n",
    "#     for batch_idx, (src, target, rotation_ab, translation_ab, rotation_ba, translation_ba, euler_ab, euler_ba) in enumerate(train_loader):\n",
    "    for batch_idx, content in enumerate(train_loader):\n",
    "        (src, pointcloud1, pointcloud2, pointcloud3,mat1,mat2,mat3_0,mat3_1)=content\n",
    "        src=src.to(device).float()\n",
    "#         target = target.to(device)\n",
    "        pointcloud1=pointcloud1.to(device).float()\n",
    "        pointcloud2=pointcloud2.to(device).float()\n",
    "        pointcloud3=pointcloud3.to(device).float()\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        translation_ab1,R_ab1=mat1[0],mat1[1]\n",
    "\n",
    "        translation_ab2,R_ab2=mat2[0],mat2[1]\n",
    "\n",
    "        translation_ab3_0,R_ab3_0=mat3_0[0],mat3_0[1]\n",
    "        translation_ab3_1,R_ab3_1=mat3_1[0],mat3_1[1]\n",
    "        translation_ab3=translation_ab3_0+translation_ab3_1\n",
    "        R_ab3=R_ab3_0.matmul(R_ab3_1)\n",
    "        \n",
    "        translation_ab1 = translation_ab1.to(device).float()\n",
    "        translation_ab2 = translation_ab2.to(device).float()\n",
    "        translation_ab3 = translation_ab3.to(device).float()\n",
    "        R_ab1 = R_ab1.to(device).float()\n",
    "        R_ab2 = R_ab2.to(device).float()\n",
    "        R_ab3 = R_ab3.to(device).float()\n",
    "        \n",
    "        \n",
    "        output_train1 = model(src,pointcloud1,R_ab1,translation_ab1)\n",
    "        output_train2 = model(src,pointcloud2,R_ab2,translation_ab2)\n",
    "        output_train3 = model(src,pointcloud3,R_ab3,translation_ab3)\n",
    "        \n",
    "        \n",
    "        loss1 = criterion(output_train1, pointcloud1)\n",
    "        loss2 = criterion(output_train2, pointcloud2)\n",
    "        loss3 = criterion(output_train3, pointcloud3)\n",
    "        \n",
    "        loss1.backward()\n",
    "        loss2.backward()\n",
    "        loss3.backward()\n",
    "        \n",
    "        \n",
    "        loss=(loss1+loss2+loss3)\n",
    "        optimizer.step()\n",
    "        #pred_train = output_train.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "        #train_corrects += pred_train.eq(target.view_as(pred_train)).sum().item()\n",
    "#         train_error_logs.append(loss.item())\n",
    "        train_error_logs.append(loss1.item())\n",
    "        train_error_logs.append(loss2.item())\n",
    "        train_error_logs.append(loss3.item())\n",
    "        \n",
    "        if (batch_idx+0) % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime: {:.2f}'.format(\n",
    "                epoch, (batch_idx+0)* len(src), len(train_loader.dataset),\n",
    "                100. * (batch_idx+0) / len(train_loader), loss,(datetime.now()-t1).total_seconds()))\n",
    "\n",
    "        \n",
    "    print('Train Epoch: {} Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "                epoch, train_corrects, len(train_loader.dataset),\n",
    "                100. * train_corrects / len(train_loader.dataset)))\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, target, rotation_ab, translation_ab, rotation_ba, translation_ba, euler_ab, euler_ba in enumerate(test_loader):\n",
    "            src = src.to(device)\n",
    "            target = target.to(device)\n",
    "            rotation_ab = rotation_ab.to(device)\n",
    "            translation_ab = translation_ab.to(device)\n",
    "            rotation_ba = rotation_ba.to(device)\n",
    "            translation_ba = translation_ba.to(device)\n",
    "            output_test = model(src,target,rotation_ab,translation_ab)\n",
    "            test_loss += criterion(output_test, target_test).item()  # sum up batch loss\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_loss_log.append(test_loss)\n",
    "    print('Test set: Average loss: {:.8f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct_test, len(test_loader.dataset),\n",
    "            100. * correct_test / len(test_loader.dataset)))\n",
    "    return train_error_logs,test_loss_log,test_acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/9840 (0%)]\tLoss: 1886.084839\tTime: 1.25\n",
      "Train Epoch: 1 [640/9840 (7%)]\tLoss: 2.709973\tTime: 9.96\n",
      "Train Epoch: 1 [1280/9840 (13%)]\tLoss: 0.017026\tTime: 18.37\n",
      "Train Epoch: 1 [1920/9840 (20%)]\tLoss: 0.001057\tTime: 26.44\n",
      "Train Epoch: 1 [2560/9840 (26%)]\tLoss: 0.011959\tTime: 34.52\n",
      "Train Epoch: 1 [3200/9840 (33%)]\tLoss: 0.001022\tTime: 42.56\n",
      "Train Epoch: 1 [3840/9840 (39%)]\tLoss: 0.000288\tTime: 50.45\n",
      "Train Epoch: 1 [4480/9840 (46%)]\tLoss: 0.000710\tTime: 58.44\n",
      "Train Epoch: 1 [5120/9840 (52%)]\tLoss: 0.000353\tTime: 66.47\n",
      "Train Epoch: 1 [5760/9840 (59%)]\tLoss: 0.001293\tTime: 74.53\n",
      "Train Epoch: 1 [6400/9840 (65%)]\tLoss: 0.000234\tTime: 82.53\n",
      "Train Epoch: 1 [7040/9840 (72%)]\tLoss: 0.000118\tTime: 90.68\n",
      "Train Epoch: 1 [7680/9840 (78%)]\tLoss: 0.000062\tTime: 98.68\n",
      "Train Epoch: 1 [8320/9840 (85%)]\tLoss: 0.000184\tTime: 106.58\n",
      "Train Epoch: 1 [8960/9840 (91%)]\tLoss: 0.000103\tTime: 114.74\n",
      "Train Epoch: 1 [9600/9840 (98%)]\tLoss: 0.000042\tTime: 122.66\n",
      "Train Epoch: 1 Accuracy: 0/9840 (0.00%)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 8, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-9819ed784517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_acc_this\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_eval_convexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'on'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-0b4378d95857>\u001b[0m in \u001b[0;36mtrain_eval_convexity\u001b[0;34m(model, device, train_loader, test_loader, optimizer, criterion, epoch, eval_mode)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation_ab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslation_ab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation_ba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslation_ba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meuler_ab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meuler_ba\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 8, got 2)"
     ]
    }
   ],
   "source": [
    "# Convex Training procedure\n",
    "train_time= []\n",
    "train_loss = []\n",
    "train_losses.append(train_loss)\n",
    "tests = []\n",
    "test_acc = []\n",
    "test_results_exp.append(tests)\n",
    "test_accs.append(test_acc)\n",
    "ratio = 0\n",
    "for epoch in range(1,30):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    t1 = datetime.now()\n",
    "    train_error,test_error,test_acc_this = train_eval_convexity(model,device, train_loader, test_loader, optimizer,criterion,epoch,'on')\n",
    "    train_loss.extend(train_error)\n",
    "    tests.extend(test_error)\n",
    "    test_acc.extend(test_acc_this)\n",
    "    train_time.append((datetime.now()-t1).total_seconds())\n",
    "    print((datetime.now()-t1).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Training\n",
    "criterion = dummy_loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "best=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-4e69cd989599>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrain_time\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtests\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "train_time= []\n",
    "train_loss = []\n",
    "train_losses.append(train_loss)\n",
    "tests = []\n",
    "test_acc = []\n",
    "test_results_exp.append(tests)\n",
    "test_accs.append(test_acc)\n",
    "ratio = 0\n",
    "for epoch in range(1,20):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    t1 = datetime.now()\n",
    "    train_error,test_error,test_acc_this = train_eval_icp(model,device, train_loader, test_loader, optimizer,criterion,epoch,'on')\n",
    "    train_loss.extend(train_error)\n",
    "    tests.extend(test_error)\n",
    "    test_acc.extend(test_acc_this)\n",
    "    train_time.append((datetime.now()-t1).total_seconds())\n",
    "    print((datetime.now()-t1).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n",
      "(3, 2048) (2048, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 8, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-41055dc7bb73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrotation_ab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslation_ab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrotation_ba\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslation_ba\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meuler_ab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meuler_ba\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 8, got 4)"
     ]
    }
   ],
   "source": [
    "for batch_idx, (src, target, rotation_ab, translation_ab, rotation_ba, translation_ba, euler_ab, euler_ba) in enumerate(train_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_ab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
