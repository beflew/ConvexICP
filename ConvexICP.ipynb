{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from data import ModelNet40,download,load_data\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from geomloss import SamplesLoss\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import warnings\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torch import jit\n",
    "%matplotlib inline\n",
    "import os\n",
    "from torch.utils.data import Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_results_exp = []\n",
    "test_accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 2 every 30 epochs\"\"\"\n",
    "    lrt = lr * (0.5 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gwu/ConvexICP/data.py:36: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  f = h5py.File(h5_name)\n"
     ]
    }
   ],
   "source": [
    "data = ModelNet40(num_points=2048, partition='train', gaussian_noise=False,\n",
    "                       unseen=False, factor=4)\n",
    "data_test = ModelNet40(num_points=2048, partition='test', gaussian_noise=False,\n",
    "                       unseen=False, factor=4)\n",
    "train_loader = DataLoader(data, batch_size=128, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(data_test, batch_size=128, shuffle=True, drop_last=True)\n",
    "rl_loader = DataLoader(data_test, batch_size=1, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_icp(model,device, train_loader,test_loader, optimizer,criterion, epoch, eval_mode='on'):\n",
    "    global best\n",
    "    model.train()\n",
    "    final_cal = nn.Sigmoid()\n",
    "    train_error_logs = []\n",
    "    t1 = datetime.now()\n",
    "    test_loss_log = []\n",
    "    test_acc_log = []\n",
    "    train_corrects = 0\n",
    "    \n",
    "    for batch_idx, (src, target, rotation_ab, translation_ab, rotation_ba, translation_ba, euler_ab, euler_ba) in enumerate(train_loader):\n",
    "        src = src.to(device)\n",
    "        target = target.to(device)\n",
    "        rotation_ab = rotation_ab.to(device)\n",
    "        translation_ab = translation_ab.to(device)\n",
    "        rotation_ba = rotation_ba.to(device)\n",
    "        translation_ba = translation_ba.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output_train = model(src,target,rotation_ab,translation_ab)\n",
    "        loss = criterion(output_train, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #pred_train = output_train.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "        #train_corrects += pred_train.eq(target.view_as(pred_train)).sum().item()\n",
    "        train_error_logs.append(loss.item())\n",
    "        \n",
    "        if (batch_idx+0) % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime: {:.2f}'.format(\n",
    "                epoch, (batch_idx+0)* len(target), len(train_loader.dataset),\n",
    "                100. * (batch_idx+0) / len(train_loader), loss,(datetime.now()-t1).total_seconds()))\n",
    "\n",
    "        \n",
    "    print('Train Epoch: {} Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "                epoch, train_corrects, len(train_loader.dataset),\n",
    "                100. * train_corrects / len(train_loader.dataset)))\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_test = 0\n",
    "    with torch.no_grad():\n",
    "        for src, target, rotation_ab, translation_ab, rotation_ba, translation_ba, euler_ab, euler_ba in test_loader:\n",
    "            src = src.to(device)\n",
    "            target = target.to(device)\n",
    "            rotation_ab = rotation_ab.to(device)\n",
    "            translation_ab = translation_ab.to(device)\n",
    "            rotation_ba = rotation_ba.to(device)\n",
    "            translation_ba = translation_ba.to(device)\n",
    "            output_test = model(src,target,rotation_ab,translation_ab)\n",
    "            test_loss += criterion(output_test, target).item()*len(src)  # sum up batch loss\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_loss_log.append(test_loss)\n",
    "    print('Test set: Average loss: {:.8f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct_test, len(test_loader.dataset),\n",
    "            100. * correct_test / len(test_loader.dataset)))\n",
    "    return train_error_logs,test_loss_log,test_acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_rt_loss(ra,ta,rb,tb):\n",
    "    r_loss = F.mse_loss(ra,rb).item()\n",
    "    t_loss = F.mse_loss(ta,tb).item()\n",
    "    return r_loss, t_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet(nn.Module):\n",
    "    def __init__(self, emb_dims=512):\n",
    "        super(PointNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(3, 64, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n",
    "        self.conv3 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n",
    "        self.conv4 = nn.Conv1d(64, 128, kernel_size=1, bias=False)\n",
    "        self.conv5 = nn.Conv1d(128, emb_dims, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.bn5 = nn.BatchNorm1d(emb_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPNet(nn.Module):\n",
    "    def __init__(self, emb_dims=512):\n",
    "        super(CPNet, self).__init__()\n",
    "        self.base_net = PointNet()\n",
    "        self.loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.05)\n",
    "\n",
    "        \n",
    "    def forward(self, x1, x2, R, T):\n",
    "        \n",
    "        trans = torch.matmul(R, x1) + T.unsqueeze(2)\n",
    "        emb1 = self.base_net(trans)\n",
    "        emb2 = self.base_net(x2)\n",
    "        \n",
    "        dist = self.loss(emb1,emb2)\n",
    "\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(1,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPEvalNet(nn.Module):\n",
    "    def __init__(self, base_net, emb_dims=512):\n",
    "        super(CPEvalNet, self).__init__()\n",
    "        self.base_net = PointNet()\n",
    "        self.base_net.load_state_dict(base_net.state_dict())\n",
    "        for param in self.base_net.parameters():\n",
    "            param.require_grad = False\n",
    "        self.rotation = nn.Parameter(torch.Tensor(1,3,3))\n",
    "        self.translation = nn.Parameter(torch.Tensor(1,3))\n",
    "        nn.init.kaiming_uniform_(self.rotation, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.translation, a=math.sqrt(5))\n",
    "        self.loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.05)\n",
    "\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        \n",
    "        trans = torch.matmul(self.rotation, x1) + self.translation.unsqueeze(2)\n",
    "        emb1 = self.base_net(trans)\n",
    "        emb2 = self.base_net(x2)\n",
    "        \n",
    "        dist = self.loss(emb1,emb2)\n",
    "\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_loss(output, target):\n",
    "    loss = torch.mean(output)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CPNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = dummy_loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "best=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/9840 (0%)]\tLoss: 0.000088\tTime: 0.54\n",
      "Train Epoch: 1 [2560/9840 (26%)]\tLoss: 0.000006\tTime: 9.78\n",
      "Train Epoch: 1 [5120/9840 (53%)]\tLoss: 0.000062\tTime: 19.06\n",
      "Train Epoch: 1 [7680/9840 (79%)]\tLoss: 0.000014\tTime: 28.31\n",
      "Train Epoch: 1 Accuracy: 0/9840 (0.00%)\n",
      "\n",
      "Test set: Average loss: 0.00004385, Accuracy: 0/2468 (0.00%)\n",
      "\n",
      "40.846422\n",
      "Train Epoch: 2 [0/9840 (0%)]\tLoss: 0.000008\tTime: 0.46\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-ce900769195e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_acc_this\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_eval_icp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'on'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-37abf57c3673>\u001b[0m in \u001b[0;36mtrain_eval_icp\u001b[0;34m(model, device, train_loader, test_loader, optimizer, criterion, epoch, eval_mode)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#pred_train = output_train.argmax(dim=1, keepdim=True) # get the index of the max log-probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#train_corrects += pred_train.eq(target.view_as(pred_train)).sum().item()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtrain_error_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_time= []\n",
    "train_loss = []\n",
    "train_losses.append(train_loss)\n",
    "tests = []\n",
    "test_acc = []\n",
    "test_results_exp.append(tests)\n",
    "test_accs.append(test_acc)\n",
    "ratio = 0\n",
    "for epoch in range(1,600):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    t1 = datetime.now()\n",
    "    train_error,test_error,test_acc_this = train_eval_icp(model,device, train_loader, test_loader, optimizer,criterion,epoch,'on')\n",
    "    train_loss.extend(train_error)\n",
    "    tests.extend(test_error)\n",
    "    test_acc.extend(test_acc_this)\n",
    "    train_time.append((datetime.now()-t1).total_seconds())\n",
    "    print((datetime.now()-t1).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rt_learning(model, device, src, target, rotation_ab, translation_ab, optimizer,criterion,epoch,n_batch = 40):\n",
    "    r_losses = []\n",
    "    t_losses = []\n",
    "    for _ in range(n_batch):\n",
    "        model.train()\n",
    "        t1 = datetime.now()\n",
    "        src = src.to(device)\n",
    "        target = target.to(device)\n",
    "        rotation_ab = rotation_ab.to(device)\n",
    "        translation_ab = translation_ab.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output_train = model(src,target)\n",
    "        loss = criterion(output_train, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        r_loss,t_loss = cal_rt_loss(model.rotation, model.translation,rotation_ab, translation_ab)\n",
    "        r_losses.append(r_loss)\n",
    "        t_losses.append(t_loss)\n",
    "    return r_losses, t_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_losses = []\n",
    "t_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval = CPEvalNet(model.base_net).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = dummy_loss\n",
    "lr_rl = 1e10\n",
    "optimizer = torch.optim.SGD(model_eval.parameters(), lr_rl,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "best=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for src_rl, target_rl, rotation_ab_rl, translation_ab_rl, rotation_ba_rl, translation_ba_rl,_,_ in rl_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 \tRotation_Loss: 0.456901\tTranslation_Loss: 0.005491\tTime: 1.00\n",
      "Train Epoch: 2 \tRotation_Loss: 0.456847\tTranslation_Loss: 0.005488\tTime: 0.98\n",
      "Train Epoch: 3 \tRotation_Loss: 0.456792\tTranslation_Loss: 0.005485\tTime: 0.99\n",
      "Train Epoch: 4 \tRotation_Loss: 0.456738\tTranslation_Loss: 0.005482\tTime: 0.98\n",
      "Train Epoch: 5 \tRotation_Loss: 0.456684\tTranslation_Loss: 0.005479\tTime: 0.98\n",
      "Train Epoch: 6 \tRotation_Loss: 0.456631\tTranslation_Loss: 0.005476\tTime: 0.99\n",
      "Train Epoch: 7 \tRotation_Loss: 0.456578\tTranslation_Loss: 0.005473\tTime: 0.99\n",
      "Train Epoch: 8 \tRotation_Loss: 0.456524\tTranslation_Loss: 0.005471\tTime: 1.01\n",
      "Train Epoch: 9 \tRotation_Loss: 0.456471\tTranslation_Loss: 0.005468\tTime: 0.99\n",
      "Train Epoch: 10 \tRotation_Loss: 0.456418\tTranslation_Loss: 0.005465\tTime: 0.97\n",
      "Train Epoch: 11 \tRotation_Loss: 0.456366\tTranslation_Loss: 0.005462\tTime: 1.01\n",
      "Train Epoch: 12 \tRotation_Loss: 0.456314\tTranslation_Loss: 0.005459\tTime: 0.99\n",
      "Train Epoch: 13 \tRotation_Loss: 0.456262\tTranslation_Loss: 0.005456\tTime: 0.99\n",
      "Train Epoch: 14 \tRotation_Loss: 0.456210\tTranslation_Loss: 0.005453\tTime: 1.00\n",
      "Train Epoch: 15 \tRotation_Loss: 0.456158\tTranslation_Loss: 0.005451\tTime: 1.00\n",
      "Train Epoch: 16 \tRotation_Loss: 0.456106\tTranslation_Loss: 0.005448\tTime: 0.99\n",
      "Train Epoch: 17 \tRotation_Loss: 0.456054\tTranslation_Loss: 0.005445\tTime: 0.99\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-1ed7a6b29122>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     r_batch_loss, t_batch_loss = rt_learning(model_eval, device, src_rl, target_rl, rotation_ab_rl, translation_ab_rl, \n\u001b[0m\u001b[1;32m     11\u001b[0m                                      optimizer,criterion,epoch,n_batch=40)\n\u001b[1;32m     12\u001b[0m     \u001b[0mr_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_batch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-3f8591e2c5dc>\u001b[0m in \u001b[0;36mrt_learning\u001b[0;34m(model, device, src, target, rotation_ab, translation_ab, optimizer, criterion, epoch, n_batch)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtranslation_ab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslation_ab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutput_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-029868469769>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0memb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/geomloss/samples_loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;31m# Run --------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         values = routines[self.loss][backend](\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0mα\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/geomloss/sinkhorn_samples.py\u001b[0m in \u001b[0;36msinkhorn_tensorized\u001b[0;34m(α, x, β, y, p, blur, reach, diameter, scaling, cost, debias, potentials, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mdiameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mε\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mε_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mρ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaling_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreach\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     a_x, b_y, a_y, b_x = sinkhorn_loop(\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0msoftmin_tensorized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mlog_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mα\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/geomloss/sinkhorn_divergence.py\u001b[0m in \u001b[0;36msinkhorn_loop\u001b[0;34m(softmin, α_logs, β_logs, C_xxs, C_yys, C_xys, C_yxs, ε_s, ρ, jumps, kernel_truncation, truncate, cost, extrapolate, debias, last_extrapolation)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# Symmetrized updates:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdebias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0ma_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mat_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbt_y\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# OT(α,α), OT(β,β)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0ma_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mat_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbt_x\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# OT(α,β) wrt. a, b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_time= []\n",
    "r_loss = []\n",
    "t_loss = []\n",
    "r_losses.append(r_loss)\n",
    "t_losses.append(t_loss)\n",
    "ratio = 0\n",
    "for epoch in range(1,600):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    t1 = datetime.now()\n",
    "    r_batch_loss, t_batch_loss = rt_learning(model_eval, device, src_rl, target_rl, rotation_ab_rl, translation_ab_rl, \n",
    "                                     optimizer,criterion,epoch,n_batch=40)\n",
    "    r_loss.extend(r_batch_loss)\n",
    "    t_loss.extend(t_batch_loss)\n",
    "    train_time.append((datetime.now()-t1).total_seconds())\n",
    "\n",
    "    print('Train Epoch: {} \\tRotation_Loss: {:.6f}\\tTranslation_Loss: {:.6f}\\tTime: {:.2f}'.format(\n",
    "        epoch, r_batch_loss[-1],t_batch_loss[-1],(datetime.now()-t1).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-0.1170,  0.7892,  0.4633],\n",
       "         [ 0.2884, -0.0550,  0.0122],\n",
       "         [-0.3263,  0.0933,  0.3119]]], device='cuda:2', requires_grad=True)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval.rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-0.1176,  0.7919,  0.4648],\n",
       "         [ 0.2894, -0.0554,  0.0123],\n",
       "         [-0.3272,  0.0937,  0.3132]]], device='cuda:2', requires_grad=True)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval.rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8242, -0.5311,  0.1967],\n",
       "         [ 0.5662,  0.7817, -0.2614],\n",
       "         [-0.0149,  0.3268,  0.9450]]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotation_ab_rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
