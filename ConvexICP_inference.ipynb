{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from data import ModelNet40,download,load_data\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from geomloss import SamplesLoss\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import warnings\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torch import jit\n",
    "%matplotlib inline\n",
    "import os\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_results_exp = []\n",
    "test_accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 2 every 30 epochs\"\"\"\n",
    "    lrt = lr * (0.5 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ModelNet40(num_points=2048, partition='train', gaussian_noise=False,\n",
    "                       unseen=False, factor=4)\n",
    "data_test = ModelNet40(num_points=2048, partition='test', gaussian_noise=False,\n",
    "                       unseen=True, factor=4)\n",
    "train_loader = DataLoader(data, batch_size=128, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(data_test, batch_size=128, shuffle=True, drop_last=True)\n",
    "rl_loader = DataLoader(data_test, batch_size=1, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_rt_loss(ra,ta,rb,tb):\n",
    "    r_loss = F.mse_loss(ra,rb).item()\n",
    "    t_loss = F.mse_loss(ta,tb).item()\n",
    "    return r_loss, t_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet(nn.Module):\n",
    "    def __init__(self, emb_dims=512):\n",
    "        super(PointNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(3, 64, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n",
    "        self.conv3 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n",
    "        self.conv4 = nn.Conv1d(64, 128, kernel_size=1, bias=False)\n",
    "        self.conv5 = nn.Conv1d(128, emb_dims, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.bn5 = nn.BatchNorm1d(emb_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPNet(nn.Module):\n",
    "    def __init__(self, emb_dims=512,split = 10):\n",
    "        super(CPNet, self).__init__()\n",
    "        self.split = split\n",
    "        self.base_net = PointNet()\n",
    "        self.loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.05)\n",
    "\n",
    "        \n",
    "    def forward(self, x1, x2, R, T, R_pert, T_pert):\n",
    "        \n",
    "        trans = torch.matmul(R, x1) + T.unsqueeze(2)\n",
    "        emb1 = self.base_net(trans)\n",
    "        emb2 = self.base_net(x2)\n",
    "        \n",
    "        dist = self.loss(emb1,emb2)\n",
    "        \n",
    "        pert_out = torch.matmul(R_pert.permute(1,0,2,3), x1) + T_pert.permute(1,0,2).unsqueeze(3)\n",
    "        pert_out = pert_out.reshape(-1, pert_out.shape[-2],pert_out.shape[-1])\n",
    "        pert_emb1 = self.base_net(pert_out)\n",
    "        pert_emb2 = tile(emb2,0,self.split)\n",
    "        pert_dist = self.loss(pert_emb1,pert_emb2)/self.split\n",
    "        pert_dist = pert_dist.reshape(self.split, emb2.shape[0]).sum(0)\n",
    "        \n",
    "\n",
    "        return dist,pert_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPEvalNet(nn.Module):\n",
    "    def __init__(self, base_net, emb_dims=512):\n",
    "        super(CPEvalNet, self).__init__()\n",
    "        self.base_net = PointNet()\n",
    "        self.base_net.load_state_dict(base_net.state_dict())\n",
    "        for param in self.base_net.parameters():\n",
    "            param.require_grad = False\n",
    "        self.rotation = nn.Parameter(torch.Tensor(1,3,3))\n",
    "        self.translation = nn.Parameter(torch.Tensor(1,3))\n",
    "        nn.init.kaiming_uniform_(self.rotation, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.translation, a=math.sqrt(5))\n",
    "        self.loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.05)\n",
    "\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        \n",
    "        trans = torch.matmul(self.rotation, x1) + self.translation.unsqueeze(2)\n",
    "        emb1 = self.base_net(trans)\n",
    "        emb2 = self.base_net(x2)\n",
    "        \n",
    "        dist = self.loss(emb1,emb2)\n",
    "\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = torch.load(\"pert_model.mdl\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_loss(output, target):\n",
    "    loss = torch.mean(output)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rt_learning(model, device, src, target, rotation_ab, translation_ab, optimizer,criterion,epoch,n_batch = 40):\n",
    "    r_losses = []\n",
    "    t_losses = []\n",
    "    for _ in range(n_batch):\n",
    "        model.train()\n",
    "        t1 = datetime.now()\n",
    "        src = src.to(device)\n",
    "        target = target.to(device)\n",
    "        rotation_ab = rotation_ab.to(device)\n",
    "        translation_ab = translation_ab.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output_train = model(src,target)\n",
    "        loss = criterion(output_train, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        r_loss,t_loss = cal_rt_loss(model.rotation, model.translation,rotation_ab, translation_ab)\n",
    "        r_losses.append(r_loss)\n",
    "        t_losses.append(t_loss)\n",
    "    return r_losses, t_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_losses = []\n",
    "t_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval = CPEvalNet(base_model.base_net).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = dummy_loss\n",
    "lr = 1e-1\n",
    "momentum=0.9\n",
    "weight_decay = 5e-4\n",
    "optimizer = torch.optim.SGD(model_eval.parameters(), lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "best=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for src_rl, target_rl, rotation_ab_rl, translation_ab_rl, rotation_ba_rl, translation_ba_rl,_,_ in rl_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 \tRotation_Loss: 143.610825\tTranslation_Loss: 0.272337\tTime: 1.02\n",
      "Train Epoch: 2 \tRotation_Loss: 142.305649\tTranslation_Loss: 0.268373\tTime: 0.82\n",
      "Train Epoch: 3 \tRotation_Loss: 136.947861\tTranslation_Loss: 0.264535\tTime: 0.78\n",
      "Train Epoch: 4 \tRotation_Loss: 131.736603\tTranslation_Loss: 0.260830\tTime: 0.76\n",
      "Train Epoch: 5 \tRotation_Loss: 126.726303\tTranslation_Loss: 0.257256\tTime: 0.77\n",
      "Train Epoch: 6 \tRotation_Loss: 121.910072\tTranslation_Loss: 0.253806\tTime: 0.77\n",
      "Train Epoch: 7 \tRotation_Loss: 117.280373\tTranslation_Loss: 0.250476\tTime: 0.82\n",
      "Train Epoch: 8 \tRotation_Loss: 112.829865\tTranslation_Loss: 0.247263\tTime: 0.91\n",
      "Train Epoch: 9 \tRotation_Loss: 108.551552\tTranslation_Loss: 0.244160\tTime: 0.81\n",
      "Train Epoch: 10 \tRotation_Loss: 104.438744\tTranslation_Loss: 0.241165\tTime: 0.79\n",
      "Train Epoch: 11 \tRotation_Loss: 100.484970\tTranslation_Loss: 0.238274\tTime: 0.75\n",
      "Train Epoch: 12 \tRotation_Loss: 96.684029\tTranslation_Loss: 0.235482\tTime: 0.77\n",
      "Train Epoch: 13 \tRotation_Loss: 93.029884\tTranslation_Loss: 0.232786\tTime: 0.74\n",
      "Train Epoch: 14 \tRotation_Loss: 89.516945\tTranslation_Loss: 0.230183\tTime: 0.76\n",
      "Train Epoch: 15 \tRotation_Loss: 86.139626\tTranslation_Loss: 0.227669\tTime: 0.77\n",
      "Train Epoch: 16 \tRotation_Loss: 82.892639\tTranslation_Loss: 0.225241\tTime: 0.77\n",
      "Train Epoch: 17 \tRotation_Loss: 79.770920\tTranslation_Loss: 0.222896\tTime: 0.77\n",
      "Train Epoch: 18 \tRotation_Loss: 76.769585\tTranslation_Loss: 0.220630\tTime: 0.76\n",
      "Train Epoch: 19 \tRotation_Loss: 73.883934\tTranslation_Loss: 0.218442\tTime: 0.78\n",
      "Train Epoch: 20 \tRotation_Loss: 71.109451\tTranslation_Loss: 0.216327\tTime: 0.75\n",
      "Train Epoch: 21 \tRotation_Loss: 68.441833\tTranslation_Loss: 0.214284\tTime: 0.75\n",
      "Train Epoch: 22 \tRotation_Loss: 65.876907\tTranslation_Loss: 0.212310\tTime: 0.77\n",
      "Train Epoch: 23 \tRotation_Loss: 63.410686\tTranslation_Loss: 0.210402\tTime: 0.78\n",
      "Train Epoch: 24 \tRotation_Loss: 61.039295\tTranslation_Loss: 0.208559\tTime: 0.78\n",
      "Train Epoch: 25 \tRotation_Loss: 58.759068\tTranslation_Loss: 0.206776\tTime: 0.79\n",
      "Train Epoch: 26 \tRotation_Loss: 56.566460\tTranslation_Loss: 0.205054\tTime: 0.78\n",
      "Train Epoch: 27 \tRotation_Loss: 54.458076\tTranslation_Loss: 0.203389\tTime: 0.79\n",
      "Train Epoch: 28 \tRotation_Loss: 52.430611\tTranslation_Loss: 0.201779\tTime: 0.77\n",
      "Train Epoch: 29 \tRotation_Loss: 50.480938\tTranslation_Loss: 0.200222\tTime: 0.77\n",
      "Train Epoch: 30 \tRotation_Loss: 48.606018\tTranslation_Loss: 0.198717\tTime: 0.78\n",
      "Train Epoch: 31 \tRotation_Loss: 46.802967\tTranslation_Loss: 0.197261\tTime: 0.76\n",
      "Train Epoch: 32 \tRotation_Loss: 45.068989\tTranslation_Loss: 0.195853\tTime: 0.76\n",
      "Train Epoch: 33 \tRotation_Loss: 43.401390\tTranslation_Loss: 0.194492\tTime: 0.77\n",
      "Train Epoch: 34 \tRotation_Loss: 41.797600\tTranslation_Loss: 0.193174\tTime: 0.77\n",
      "Train Epoch: 35 \tRotation_Loss: 40.255146\tTranslation_Loss: 0.191900\tTime: 0.77\n",
      "Train Epoch: 36 \tRotation_Loss: 38.771645\tTranslation_Loss: 0.190667\tTime: 0.76\n",
      "Train Epoch: 37 \tRotation_Loss: 37.344810\tTranslation_Loss: 0.189475\tTime: 0.77\n",
      "Train Epoch: 38 \tRotation_Loss: 35.972454\tTranslation_Loss: 0.188320\tTime: 0.76\n",
      "Train Epoch: 39 \tRotation_Loss: 34.652454\tTranslation_Loss: 0.187203\tTime: 0.77\n",
      "Train Epoch: 40 \tRotation_Loss: 33.382778\tTranslation_Loss: 0.186122\tTime: 0.76\n",
      "Train Epoch: 41 \tRotation_Loss: 32.161488\tTranslation_Loss: 0.185076\tTime: 0.76\n",
      "Train Epoch: 42 \tRotation_Loss: 30.986704\tTranslation_Loss: 0.184063\tTime: 0.78\n",
      "Train Epoch: 43 \tRotation_Loss: 29.856632\tTranslation_Loss: 0.183082\tTime: 0.77\n",
      "Train Epoch: 44 \tRotation_Loss: 28.769527\tTranslation_Loss: 0.182132\tTime: 0.83\n",
      "Train Epoch: 45 \tRotation_Loss: 27.723736\tTranslation_Loss: 0.181213\tTime: 0.75\n",
      "Train Epoch: 46 \tRotation_Loss: 26.717670\tTranslation_Loss: 0.180322\tTime: 0.71\n",
      "Train Epoch: 47 \tRotation_Loss: 25.749783\tTranslation_Loss: 0.179460\tTime: 0.73\n",
      "Train Epoch: 48 \tRotation_Loss: 24.818602\tTranslation_Loss: 0.178624\tTime: 0.72\n",
      "Train Epoch: 49 \tRotation_Loss: 23.922710\tTranslation_Loss: 0.177815\tTime: 0.74\n",
      "Train Epoch: 50 \tRotation_Loss: 23.060743\tTranslation_Loss: 0.177031\tTime: 0.78\n",
      "Train Epoch: 51 \tRotation_Loss: 22.231398\tTranslation_Loss: 0.176272\tTime: 0.77\n",
      "Train Epoch: 52 \tRotation_Loss: 21.433409\tTranslation_Loss: 0.175536\tTime: 0.77\n",
      "Train Epoch: 53 \tRotation_Loss: 20.665569\tTranslation_Loss: 0.174823\tTime: 0.75\n",
      "Train Epoch: 54 \tRotation_Loss: 19.926714\tTranslation_Loss: 0.174131\tTime: 0.77\n",
      "Train Epoch: 55 \tRotation_Loss: 19.215733\tTranslation_Loss: 0.173461\tTime: 0.76\n",
      "Train Epoch: 56 \tRotation_Loss: 18.531548\tTranslation_Loss: 0.172812\tTime: 0.76\n",
      "Train Epoch: 57 \tRotation_Loss: 17.873133\tTranslation_Loss: 0.172182\tTime: 0.76\n",
      "Train Epoch: 58 \tRotation_Loss: 17.239485\tTranslation_Loss: 0.171572\tTime: 0.76\n",
      "Train Epoch: 59 \tRotation_Loss: 16.629663\tTranslation_Loss: 0.170980\tTime: 0.75\n",
      "Train Epoch: 60 \tRotation_Loss: 16.042749\tTranslation_Loss: 0.170406\tTime: 0.75\n",
      "Train Epoch: 61 \tRotation_Loss: 15.477858\tTranslation_Loss: 0.169849\tTime: 0.77\n",
      "Train Epoch: 62 \tRotation_Loss: 14.934152\tTranslation_Loss: 0.169309\tTime: 0.77\n",
      "Train Epoch: 63 \tRotation_Loss: 14.410806\tTranslation_Loss: 0.168785\tTime: 0.77\n",
      "Train Epoch: 64 \tRotation_Loss: 13.907047\tTranslation_Loss: 0.168277\tTime: 0.76\n",
      "Train Epoch: 65 \tRotation_Loss: 13.422124\tTranslation_Loss: 0.167784\tTime: 0.76\n",
      "Train Epoch: 66 \tRotation_Loss: 12.955311\tTranslation_Loss: 0.167305\tTime: 0.76\n",
      "Train Epoch: 67 \tRotation_Loss: 12.505918\tTranslation_Loss: 0.166841\tTime: 0.75\n",
      "Train Epoch: 68 \tRotation_Loss: 12.073277\tTranslation_Loss: 0.166390\tTime: 0.75\n",
      "Train Epoch: 69 \tRotation_Loss: 11.656746\tTranslation_Loss: 0.165953\tTime: 0.77\n",
      "Train Epoch: 70 \tRotation_Loss: 11.255709\tTranslation_Loss: 0.165528\tTime: 0.76\n",
      "Train Epoch: 71 \tRotation_Loss: 10.869575\tTranslation_Loss: 0.165116\tTime: 0.77\n",
      "Train Epoch: 72 \tRotation_Loss: 10.497777\tTranslation_Loss: 0.164716\tTime: 0.75\n",
      "Train Epoch: 73 \tRotation_Loss: 10.139764\tTranslation_Loss: 0.164327\tTime: 0.77\n",
      "Train Epoch: 74 \tRotation_Loss: 9.795009\tTranslation_Loss: 0.163950\tTime: 0.76\n",
      "Train Epoch: 75 \tRotation_Loss: 9.463010\tTranslation_Loss: 0.163583\tTime: 0.76\n",
      "Train Epoch: 76 \tRotation_Loss: 9.143280\tTranslation_Loss: 0.163227\tTime: 0.78\n",
      "Train Epoch: 77 \tRotation_Loss: 8.835351\tTranslation_Loss: 0.162881\tTime: 0.75\n",
      "Train Epoch: 78 \tRotation_Loss: 8.538774\tTranslation_Loss: 0.162545\tTime: 0.77\n",
      "Train Epoch: 79 \tRotation_Loss: 8.253119\tTranslation_Loss: 0.162219\tTime: 0.84\n",
      "Train Epoch: 80 \tRotation_Loss: 7.977968\tTranslation_Loss: 0.161902\tTime: 0.87\n",
      "Train Epoch: 81 \tRotation_Loss: 7.712922\tTranslation_Loss: 0.161593\tTime: 0.74\n",
      "Train Epoch: 82 \tRotation_Loss: 7.457602\tTranslation_Loss: 0.161294\tTime: 0.73\n",
      "Train Epoch: 83 \tRotation_Loss: 7.211634\tTranslation_Loss: 0.161002\tTime: 0.73\n",
      "Train Epoch: 84 \tRotation_Loss: 6.974665\tTranslation_Loss: 0.160719\tTime: 0.73\n",
      "Train Epoch: 85 \tRotation_Loss: 6.746355\tTranslation_Loss: 0.160444\tTime: 0.75\n",
      "Train Epoch: 86 \tRotation_Loss: 6.526377\tTranslation_Loss: 0.160177\tTime: 0.76\n",
      "Train Epoch: 87 \tRotation_Loss: 6.314415\tTranslation_Loss: 0.159916\tTime: 0.76\n",
      "Train Epoch: 88 \tRotation_Loss: 6.110167\tTranslation_Loss: 0.159663\tTime: 0.74\n",
      "Train Epoch: 89 \tRotation_Loss: 5.913341\tTranslation_Loss: 0.159417\tTime: 0.76\n",
      "Train Epoch: 90 \tRotation_Loss: 5.723657\tTranslation_Loss: 0.159178\tTime: 0.76\n",
      "Train Epoch: 91 \tRotation_Loss: 5.540845\tTranslation_Loss: 0.158945\tTime: 0.75\n",
      "Train Epoch: 92 \tRotation_Loss: 5.364649\tTranslation_Loss: 0.158718\tTime: 0.76\n",
      "Train Epoch: 93 \tRotation_Loss: 5.194818\tTranslation_Loss: 0.158498\tTime: 0.77\n",
      "Train Epoch: 94 \tRotation_Loss: 5.031112\tTranslation_Loss: 0.158284\tTime: 0.78\n",
      "Train Epoch: 95 \tRotation_Loss: 4.873301\tTranslation_Loss: 0.158075\tTime: 0.76\n",
      "Train Epoch: 96 \tRotation_Loss: 4.721166\tTranslation_Loss: 0.157872\tTime: 0.76\n",
      "Train Epoch: 97 \tRotation_Loss: 4.574492\tTranslation_Loss: 0.157674\tTime: 0.76\n",
      "Train Epoch: 98 \tRotation_Loss: 4.433075\tTranslation_Loss: 0.157482\tTime: 0.76\n",
      "Train Epoch: 99 \tRotation_Loss: 4.296719\tTranslation_Loss: 0.157294\tTime: 0.77\n",
      "Train Epoch: 100 \tRotation_Loss: 4.165234\tTranslation_Loss: 0.157112\tTime: 0.76\n",
      "Train Epoch: 101 \tRotation_Loss: 4.038439\tTranslation_Loss: 0.156935\tTime: 0.76\n",
      "Train Epoch: 102 \tRotation_Loss: 3.916158\tTranslation_Loss: 0.156762\tTime: 0.75\n",
      "Train Epoch: 103 \tRotation_Loss: 3.798223\tTranslation_Loss: 0.156593\tTime: 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 104 \tRotation_Loss: 3.684473\tTranslation_Loss: 0.156429\tTime: 0.76\n",
      "Train Epoch: 105 \tRotation_Loss: 3.574750\tTranslation_Loss: 0.156270\tTime: 0.74\n",
      "Train Epoch: 106 \tRotation_Loss: 3.468908\tTranslation_Loss: 0.156114\tTime: 0.77\n",
      "Train Epoch: 107 \tRotation_Loss: 3.366798\tTranslation_Loss: 0.155963\tTime: 0.74\n",
      "Train Epoch: 108 \tRotation_Loss: 3.268287\tTranslation_Loss: 0.155815\tTime: 0.76\n",
      "Train Epoch: 109 \tRotation_Loss: 3.173238\tTranslation_Loss: 0.155671\tTime: 0.76\n",
      "Train Epoch: 110 \tRotation_Loss: 3.081525\tTranslation_Loss: 0.155531\tTime: 0.76\n",
      "Train Epoch: 111 \tRotation_Loss: 2.993024\tTranslation_Loss: 0.155395\tTime: 0.75\n",
      "Train Epoch: 112 \tRotation_Loss: 2.907615\tTranslation_Loss: 0.155261\tTime: 0.76\n",
      "Train Epoch: 113 \tRotation_Loss: 2.825186\tTranslation_Loss: 0.155132\tTime: 0.76\n",
      "Train Epoch: 114 \tRotation_Loss: 2.745627\tTranslation_Loss: 0.155005\tTime: 0.76\n",
      "Train Epoch: 115 \tRotation_Loss: 2.668831\tTranslation_Loss: 0.154882\tTime: 0.76\n",
      "Train Epoch: 116 \tRotation_Loss: 2.594699\tTranslation_Loss: 0.154762\tTime: 0.75\n",
      "Train Epoch: 117 \tRotation_Loss: 2.523130\tTranslation_Loss: 0.154645\tTime: 0.78\n",
      "Train Epoch: 118 \tRotation_Loss: 2.454033\tTranslation_Loss: 0.154531\tTime: 0.84\n",
      "Train Epoch: 119 \tRotation_Loss: 2.387317\tTranslation_Loss: 0.154419\tTime: 0.91\n",
      "Train Epoch: 120 \tRotation_Loss: 2.322893\tTranslation_Loss: 0.154311\tTime: 0.76\n",
      "Train Epoch: 121 \tRotation_Loss: 2.260680\tTranslation_Loss: 0.154205\tTime: 0.70\n",
      "Train Epoch: 122 \tRotation_Loss: 2.200596\tTranslation_Loss: 0.154101\tTime: 0.75\n",
      "Train Epoch: 123 \tRotation_Loss: 2.142564\tTranslation_Loss: 0.154001\tTime: 0.73\n",
      "Train Epoch: 124 \tRotation_Loss: 2.086509\tTranslation_Loss: 0.153902\tTime: 0.75\n",
      "Train Epoch: 125 \tRotation_Loss: 2.032359\tTranslation_Loss: 0.153806\tTime: 0.77\n",
      "Train Epoch: 126 \tRotation_Loss: 1.980046\tTranslation_Loss: 0.153713\tTime: 0.76\n",
      "Train Epoch: 127 \tRotation_Loss: 1.929503\tTranslation_Loss: 0.153622\tTime: 0.75\n",
      "Train Epoch: 128 \tRotation_Loss: 1.880666\tTranslation_Loss: 0.153533\tTime: 0.77\n",
      "Train Epoch: 129 \tRotation_Loss: 1.833473\tTranslation_Loss: 0.153446\tTime: 0.75\n",
      "Train Epoch: 130 \tRotation_Loss: 1.787866\tTranslation_Loss: 0.153361\tTime: 0.75\n",
      "Train Epoch: 131 \tRotation_Loss: 1.743787\tTranslation_Loss: 0.153278\tTime: 0.77\n",
      "Train Epoch: 132 \tRotation_Loss: 1.701181\tTranslation_Loss: 0.153197\tTime: 0.75\n",
      "Train Epoch: 133 \tRotation_Loss: 1.659997\tTranslation_Loss: 0.153118\tTime: 0.76\n",
      "Train Epoch: 134 \tRotation_Loss: 1.620181\tTranslation_Loss: 0.153042\tTime: 0.78\n",
      "Train Epoch: 135 \tRotation_Loss: 1.581687\tTranslation_Loss: 0.152966\tTime: 0.78\n",
      "Train Epoch: 136 \tRotation_Loss: 1.544466\tTranslation_Loss: 0.152893\tTime: 0.84\n",
      "Train Epoch: 137 \tRotation_Loss: 1.508474\tTranslation_Loss: 0.152821\tTime: 0.77\n",
      "Train Epoch: 138 \tRotation_Loss: 1.473667\tTranslation_Loss: 0.152752\tTime: 0.72\n",
      "Train Epoch: 139 \tRotation_Loss: 1.440002\tTranslation_Loss: 0.152683\tTime: 0.76\n",
      "Train Epoch: 140 \tRotation_Loss: 1.407440\tTranslation_Loss: 0.152617\tTime: 0.72\n",
      "Train Epoch: 141 \tRotation_Loss: 1.375940\tTranslation_Loss: 0.152551\tTime: 0.77\n",
      "Train Epoch: 142 \tRotation_Loss: 1.345466\tTranslation_Loss: 0.152488\tTime: 0.76\n",
      "Train Epoch: 143 \tRotation_Loss: 1.315982\tTranslation_Loss: 0.152426\tTime: 0.77\n",
      "Train Epoch: 144 \tRotation_Loss: 1.287453\tTranslation_Loss: 0.152365\tTime: 0.77\n",
      "Train Epoch: 145 \tRotation_Loss: 1.259846\tTranslation_Loss: 0.152306\tTime: 0.78\n",
      "Train Epoch: 146 \tRotation_Loss: 1.233127\tTranslation_Loss: 0.152248\tTime: 0.75\n",
      "Train Epoch: 147 \tRotation_Loss: 1.207266\tTranslation_Loss: 0.152191\tTime: 0.76\n",
      "Train Epoch: 148 \tRotation_Loss: 1.182234\tTranslation_Loss: 0.152136\tTime: 0.76\n",
      "Train Epoch: 149 \tRotation_Loss: 1.158000\tTranslation_Loss: 0.152082\tTime: 0.76\n",
      "Train Epoch: 150 \tRotation_Loss: 1.134538\tTranslation_Loss: 0.152030\tTime: 0.77\n",
      "Train Epoch: 151 \tRotation_Loss: 1.111821\tTranslation_Loss: 0.151978\tTime: 0.76\n",
      "Train Epoch: 152 \tRotation_Loss: 1.089823\tTranslation_Loss: 0.151928\tTime: 0.77\n",
      "Train Epoch: 153 \tRotation_Loss: 1.068519\tTranslation_Loss: 0.151878\tTime: 0.75\n",
      "Train Epoch: 154 \tRotation_Loss: 1.047885\tTranslation_Loss: 0.151830\tTime: 0.75\n",
      "Train Epoch: 155 \tRotation_Loss: 1.027898\tTranslation_Loss: 0.151783\tTime: 0.80\n",
      "Train Epoch: 156 \tRotation_Loss: 1.008536\tTranslation_Loss: 0.151737\tTime: 0.75\n",
      "Train Epoch: 157 \tRotation_Loss: 0.989777\tTranslation_Loss: 0.151693\tTime: 0.77\n",
      "Train Epoch: 158 \tRotation_Loss: 0.971602\tTranslation_Loss: 0.151649\tTime: 0.77\n",
      "Train Epoch: 159 \tRotation_Loss: 0.953990\tTranslation_Loss: 0.151606\tTime: 0.77\n",
      "Train Epoch: 160 \tRotation_Loss: 0.936922\tTranslation_Loss: 0.151564\tTime: 0.76\n",
      "Train Epoch: 161 \tRotation_Loss: 0.920379\tTranslation_Loss: 0.151523\tTime: 0.77\n",
      "Train Epoch: 162 \tRotation_Loss: 0.904344\tTranslation_Loss: 0.151483\tTime: 0.76\n",
      "Train Epoch: 163 \tRotation_Loss: 0.888800\tTranslation_Loss: 0.151444\tTime: 0.76\n",
      "Train Epoch: 164 \tRotation_Loss: 0.873730\tTranslation_Loss: 0.151405\tTime: 0.75\n",
      "Train Epoch: 165 \tRotation_Loss: 0.859117\tTranslation_Loss: 0.151368\tTime: 0.76\n",
      "Train Epoch: 166 \tRotation_Loss: 0.844948\tTranslation_Loss: 0.151331\tTime: 0.75\n",
      "Train Epoch: 167 \tRotation_Loss: 0.831207\tTranslation_Loss: 0.151296\tTime: 0.75\n",
      "Train Epoch: 168 \tRotation_Loss: 0.817879\tTranslation_Loss: 0.151261\tTime: 0.75\n",
      "Train Epoch: 169 \tRotation_Loss: 0.804952\tTranslation_Loss: 0.151226\tTime: 0.74\n",
      "Train Epoch: 170 \tRotation_Loss: 0.792411\tTranslation_Loss: 0.151193\tTime: 0.77\n",
      "Train Epoch: 171 \tRotation_Loss: 0.780244\tTranslation_Loss: 0.151160\tTime: 0.76\n",
      "Train Epoch: 172 \tRotation_Loss: 0.768439\tTranslation_Loss: 0.151128\tTime: 0.76\n",
      "Train Epoch: 173 \tRotation_Loss: 0.756983\tTranslation_Loss: 0.151097\tTime: 0.83\n",
      "Train Epoch: 174 \tRotation_Loss: 0.745865\tTranslation_Loss: 0.151066\tTime: 0.92\n",
      "Train Epoch: 175 \tRotation_Loss: 0.735075\tTranslation_Loss: 0.151036\tTime: 0.73\n",
      "Train Epoch: 176 \tRotation_Loss: 0.724601\tTranslation_Loss: 0.151007\tTime: 0.73\n",
      "Train Epoch: 177 \tRotation_Loss: 0.714433\tTranslation_Loss: 0.150978\tTime: 0.72\n",
      "Train Epoch: 178 \tRotation_Loss: 0.704561\tTranslation_Loss: 0.150950\tTime: 0.73\n",
      "Train Epoch: 179 \tRotation_Loss: 0.694976\tTranslation_Loss: 0.150923\tTime: 0.76\n",
      "Train Epoch: 180 \tRotation_Loss: 0.685668\tTranslation_Loss: 0.150896\tTime: 0.78\n",
      "Train Epoch: 181 \tRotation_Loss: 0.676629\tTranslation_Loss: 0.150870\tTime: 0.68\n",
      "Train Epoch: 182 \tRotation_Loss: 0.667849\tTranslation_Loss: 0.150844\tTime: 0.66\n",
      "Train Epoch: 183 \tRotation_Loss: 0.659321\tTranslation_Loss: 0.150819\tTime: 0.66\n",
      "Train Epoch: 184 \tRotation_Loss: 0.651037\tTranslation_Loss: 0.150794\tTime: 0.66\n",
      "Train Epoch: 185 \tRotation_Loss: 0.642988\tTranslation_Loss: 0.150770\tTime: 0.66\n",
      "Train Epoch: 186 \tRotation_Loss: 0.635167\tTranslation_Loss: 0.150746\tTime: 0.66\n",
      "Train Epoch: 187 \tRotation_Loss: 0.627567\tTranslation_Loss: 0.150723\tTime: 0.66\n",
      "Train Epoch: 188 \tRotation_Loss: 0.620182\tTranslation_Loss: 0.150701\tTime: 0.66\n",
      "Train Epoch: 189 \tRotation_Loss: 0.613003\tTranslation_Loss: 0.150679\tTime: 0.66\n",
      "Train Epoch: 190 \tRotation_Loss: 0.606026\tTranslation_Loss: 0.150657\tTime: 0.66\n",
      "Train Epoch: 191 \tRotation_Loss: 0.599243\tTranslation_Loss: 0.150636\tTime: 0.66\n",
      "Train Epoch: 192 \tRotation_Loss: 0.592648\tTranslation_Loss: 0.150615\tTime: 0.66\n",
      "Train Epoch: 193 \tRotation_Loss: 0.586236\tTranslation_Loss: 0.150595\tTime: 0.66\n",
      "Train Epoch: 194 \tRotation_Loss: 0.580001\tTranslation_Loss: 0.150575\tTime: 0.66\n",
      "Train Epoch: 195 \tRotation_Loss: 0.573937\tTranslation_Loss: 0.150556\tTime: 0.66\n",
      "Train Epoch: 196 \tRotation_Loss: 0.568039\tTranslation_Loss: 0.150537\tTime: 0.67\n",
      "Train Epoch: 197 \tRotation_Loss: 0.562303\tTranslation_Loss: 0.150518\tTime: 0.66\n",
      "Train Epoch: 198 \tRotation_Loss: 0.556723\tTranslation_Loss: 0.150500\tTime: 0.66\n",
      "Train Epoch: 199 \tRotation_Loss: 0.551294\tTranslation_Loss: 0.150482\tTime: 0.66\n",
      "Train Epoch: 200 \tRotation_Loss: 0.546012\tTranslation_Loss: 0.150465\tTime: 0.66\n",
      "Train Epoch: 201 \tRotation_Loss: 0.540872\tTranslation_Loss: 0.150448\tTime: 0.66\n",
      "Train Epoch: 202 \tRotation_Loss: 0.535871\tTranslation_Loss: 0.150431\tTime: 0.66\n",
      "Train Epoch: 203 \tRotation_Loss: 0.531003\tTranslation_Loss: 0.150415\tTime: 0.67\n",
      "Train Epoch: 204 \tRotation_Loss: 0.526265\tTranslation_Loss: 0.150399\tTime: 0.66\n",
      "Train Epoch: 205 \tRotation_Loss: 0.521653\tTranslation_Loss: 0.150383\tTime: 0.66\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Maximum allowed size exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-23ea34aa992f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#adjust_learning_rate(optimizer, epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     r_batch_loss, t_batch_loss = rt_learning(model_eval, device, src_rl, target_rl, rotation_ab_rl, translation_ab_rl, \n\u001b[0m\u001b[1;32m     11\u001b[0m                                      optimizer,criterion,epoch,n_batch=40)\n\u001b[1;32m     12\u001b[0m     \u001b[0mr_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_batch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-3f8591e2c5dc>\u001b[0m in \u001b[0;36mrt_learning\u001b[0;34m(model, device, src, target, rotation_ab, translation_ab, optimizer, criterion, epoch, n_batch)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtranslation_ab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslation_ab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutput_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-029868469769>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0memb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/geomloss/samples_loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;31m# Run --------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         values = routines[self.loss][backend](\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0mα\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/geomloss/sinkhorn_samples.py\u001b[0m in \u001b[0;36msinkhorn_tensorized\u001b[0;34m(α, x, β, y, p, blur, reach, diameter, scaling, cost, debias, potentials, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mC_xy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_yx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B,N,M), (B,M,N)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mdiameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mε\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mε_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mρ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaling_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreach\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     a_x, b_y, a_y, b_x = sinkhorn_loop(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/geomloss/sinkhorn_divergence.py\u001b[0m in \u001b[0;36mscaling_parameters\u001b[0;34m(x, y, p, blur, reach, diameter, scaling)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mε\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblur\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mε_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_schedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mρ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreach\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mreach\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdiameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mε\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mε_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mρ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/geomloss/sinkhorn_divergence.py\u001b[0m in \u001b[0;36mepsilon_schedule\u001b[0;34m(p, diameter, blur, scaling)\u001b[0m\n\u001b[1;32m     67\u001b[0m         + [\n\u001b[1;32m     68\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             for e in np.arange(\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0mp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Maximum allowed size exceeded"
     ]
    }
   ],
   "source": [
    "train_time= []\n",
    "r_loss = []\n",
    "t_loss = []\n",
    "r_losses.append(r_loss)\n",
    "t_losses.append(t_loss)\n",
    "ratio = 0\n",
    "for epoch in range(1,600):\n",
    "    #adjust_learning_rate(optimizer, epoch)\n",
    "    t1 = datetime.now()\n",
    "    r_batch_loss, t_batch_loss = rt_learning(model_eval, device, src_rl, target_rl, rotation_ab_rl, translation_ab_rl, \n",
    "                                     optimizer,criterion,epoch,n_batch=40)\n",
    "    r_loss.extend(r_batch_loss)\n",
    "    t_loss.extend(t_batch_loss)\n",
    "    train_time.append((datetime.now()-t1).total_seconds())\n",
    "\n",
    "    print('Train Epoch: {} \\tRotation_Loss: {:.6f}\\tTranslation_Loss: {:.6f}\\tTime: {:.2f}'.format(\n",
    "        epoch, r_batch_loss[-1],t_batch_loss[-1],(datetime.now()-t1).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
