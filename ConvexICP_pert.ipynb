{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from data_with_pert import ModelNet40_pert,download,load_data\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from geomloss import SamplesLoss\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import warnings\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torch import jit\n",
    "%matplotlib inline\n",
    "import os\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_results_exp = []\n",
    "test_accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 2 every 30 epochs\"\"\"\n",
    "    lrt = lr * (0.5 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile(a, dim, n_tile):\n",
    "    init_dim = a.size(dim)\n",
    "    repeat_idx = [1] * a.dim()\n",
    "    repeat_idx[dim] = n_tile\n",
    "    a = a.repeat(*(repeat_idx))\n",
    "    order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)])).to(device)\n",
    "    return torch.index_select(a, dim, order_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gwu/ConvexICP/data_with_pert.py:36: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  f = h5py.File(h5_name)\n"
     ]
    }
   ],
   "source": [
    "M=10\n",
    "batch_size= 64\n",
    "data = ModelNet40_pert(num_points=1000, partition='train', gaussian_noise=False,\n",
    "                       unseen=False, factor=4,n_pert=M)\n",
    "data_test = ModelNet40_pert(num_points=1000, partition='test', gaussian_noise=False,\n",
    "                       unseen=False, factor=4,n_pert=M)\n",
    "train_loader = DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(data_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "rl_loader = DataLoader(data_test, batch_size=1, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_icp(model,device, train_loader,test_loader, optimizer,criterion, epoch, eval_mode='on'):\n",
    "    global best\n",
    "    model.train()\n",
    "    final_cal = nn.Sigmoid()\n",
    "    train_error_logs = []\n",
    "    t1 = datetime.now()\n",
    "    test_loss_log = []\n",
    "    test_acc_log = []\n",
    "    train_corrects = 0\n",
    "    \n",
    "    for batch_idx, (src, target, r_ab, t_ab, _,_,_,_,r_ab_pert,t_ab_pert) in enumerate(train_loader):\n",
    "        src = src.to(device)\n",
    "        target = target.to(device)\n",
    "        r_ab = r_ab.to(device)\n",
    "        t_ab = t_ab.to(device)\n",
    "        r_ab_pert = r_ab_pert.to(device)\n",
    "        t_ab_pert = t_ab_pert.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        dist, dist_pert = model(src,target,r_ab,t_ab,r_ab_pert,t_ab_pert)\n",
    "        loss = criterion(dist,dist_pert, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #pred_train = output_train.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "        #train_corrects += pred_train.eq(target.view_as(pred_train)).sum().item()\n",
    "        train_error_logs.append(loss.item())\n",
    "        \n",
    "        if (batch_idx+0) % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime: {:.2f}'.format(\n",
    "                epoch, (batch_idx+0)* len(target), len(train_loader.dataset),\n",
    "                100. * (batch_idx+0) / len(train_loader), loss,(datetime.now()-t1).total_seconds()))\n",
    "\n",
    "        \n",
    "    print('Train Epoch: {} Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "                epoch, train_corrects, len(train_loader.dataset),\n",
    "                100. * train_corrects / len(train_loader.dataset)))\n",
    "    '''\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_test = 0\n",
    "    with torch.no_grad():\n",
    "        for src, target, rotation_ab, translation_ab, rotation_ba, translation_ba, euler_ab, euler_ba in test_loader:\n",
    "            src = src.to(device)\n",
    "            target = target.to(device)\n",
    "            rotation_ab = rotation_ab.to(device)\n",
    "            translation_ab = translation_ab.to(device)\n",
    "            rotation_ba = rotation_ba.to(device)\n",
    "            translation_ba = translation_ba.to(device)\n",
    "            output_test = model(src,target,rotation_ab,translation_ab)\n",
    "            test_loss += criterion(output_test, target).item()*len(src)  # sum up batch loss\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_loss_log.append(test_loss)\n",
    "    print('Test set: Average loss: {:.8f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct_test, len(test_loader.dataset),\n",
    "            100. * correct_test / len(test_loader.dataset)))\n",
    "    '''\n",
    "    return train_error_logs,test_loss_log,test_acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_rt_loss(ra,ta,rb,tb):\n",
    "    r_loss = F.mse_loss(ra,rb).item()\n",
    "    t_loss = F.mse_loss(ta,tb).item()\n",
    "    return r_loss, t_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet(nn.Module):\n",
    "    def __init__(self, emb_dims=512):\n",
    "        super(PointNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(3, 64, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n",
    "        self.conv3 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n",
    "        self.conv4 = nn.Conv1d(64, 128, kernel_size=1, bias=False)\n",
    "        self.conv5 = nn.Conv1d(128, emb_dims, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.bn5 = nn.BatchNorm1d(emb_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPNet(nn.Module):\n",
    "    def __init__(self, emb_dims=512,split = 10):\n",
    "        super(CPNet, self).__init__()\n",
    "        self.split = split\n",
    "        self.base_net = PointNet()\n",
    "        self.loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.05)\n",
    "\n",
    "        \n",
    "    def forward(self, x1, x2, R, T, R_pert, T_pert):\n",
    "        \n",
    "        trans = torch.matmul(R, x1) + T.unsqueeze(2)\n",
    "        emb1 = self.base_net(trans)\n",
    "        emb2 = self.base_net(x2)\n",
    "        \n",
    "        dist = self.loss(emb1,emb2)\n",
    "        \n",
    "        pert_out = torch.matmul(R_pert.permute(1,0,2,3), x1) + T_pert.permute(1,0,2).unsqueeze(3)\n",
    "        pert_out = pert_out.reshape(-1, pert_out.shape[-2],pert_out.shape[-1])\n",
    "        pert_emb1 = self.base_net(pert_out)\n",
    "        pert_emb2 = tile(emb2,0,self.split)\n",
    "        pert_dist = self.loss(pert_emb1,pert_emb2)/self.split\n",
    "        pert_dist = pert_dist.reshape(self.split, emb2.shape[0]).sum(0)\n",
    "        \n",
    "\n",
    "        return dist,pert_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPEvalNet(nn.Module):\n",
    "    def __init__(self, base_net, emb_dims=512):\n",
    "        super(CPEvalNet, self).__init__()\n",
    "        self.base_net = PointNet()\n",
    "        self.base_net.load_state_dict(base_net.state_dict())\n",
    "        for param in self.base_net.parameters():\n",
    "            param.require_grad = False\n",
    "        self.rotation = nn.Parameter(torch.Tensor(1,3,3))\n",
    "        self.translation = nn.Parameter(torch.Tensor(1,3))\n",
    "        nn.init.kaiming_uniform_(self.rotation, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.translation, a=math.sqrt(5))\n",
    "        self.loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.05)\n",
    "\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        \n",
    "        trans = torch.matmul(self.rotation, x1) + self.translation.unsqueeze(2)\n",
    "        emb1 = self.base_net(trans)\n",
    "        emb2 = self.base_net(x2)\n",
    "        \n",
    "        dist = self.loss(emb1,emb2)\n",
    "\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_loss(dist, pert_dist, target, alpha = 1, beta = 1, eps = 0.005, eta = 0.005):\n",
    "    loss = torch.mean(dist)\n",
    "    \n",
    "    diff = dist-pert_dist\n",
    "    dummy = torch.zeros_like(diff)\n",
    "    pert_loss = F.hinge_embedding_loss(diff,dummy,margin = eps)\n",
    "    \n",
    "    return loss+alpha*pert_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CPNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = combine_loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "best=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/9840 (0%)]\tLoss: 291.557800\tTime: 1.79\n",
      "Train Epoch: 1 [1280/9840 (13%)]\tLoss: 26.751801\tTime: 33.92\n",
      "Train Epoch: 1 [2560/9840 (26%)]\tLoss: 7.135236\tTime: 64.32\n",
      "Train Epoch: 1 [3840/9840 (39%)]\tLoss: 3.205786\tTime: 93.96\n",
      "Train Epoch: 1 [5120/9840 (52%)]\tLoss: 2.022932\tTime: 123.57\n",
      "Train Epoch: 1 [6400/9840 (65%)]\tLoss: 1.257565\tTime: 153.09\n",
      "Train Epoch: 1 [7680/9840 (78%)]\tLoss: 0.871290\tTime: 182.06\n",
      "Train Epoch: 1 [8960/9840 (92%)]\tLoss: 0.664800\tTime: 210.80\n",
      "Train Epoch: 1 Accuracy: 0/9840 (0.00%)\n",
      "\n",
      "227.928147\n",
      "Train Epoch: 2 [0/9840 (0%)]\tLoss: 0.598435\tTime: 1.43\n",
      "Train Epoch: 2 [1280/9840 (13%)]\tLoss: 0.496651\tTime: 30.08\n",
      "Train Epoch: 2 [2560/9840 (26%)]\tLoss: 0.353587\tTime: 58.69\n",
      "Train Epoch: 2 [3840/9840 (39%)]\tLoss: 0.231985\tTime: 87.25\n",
      "Train Epoch: 2 [5120/9840 (52%)]\tLoss: 0.267477\tTime: 115.81\n",
      "Train Epoch: 2 [6400/9840 (65%)]\tLoss: 0.284945\tTime: 144.26\n",
      "Train Epoch: 2 [7680/9840 (78%)]\tLoss: 0.151299\tTime: 172.57\n",
      "Train Epoch: 2 [8960/9840 (92%)]\tLoss: 0.126619\tTime: 200.75\n",
      "Train Epoch: 2 Accuracy: 0/9840 (0.00%)\n",
      "\n",
      "217.775379\n",
      "Train Epoch: 3 [0/9840 (0%)]\tLoss: 0.108349\tTime: 1.37\n",
      "Train Epoch: 3 [1280/9840 (13%)]\tLoss: 0.072210\tTime: 29.12\n",
      "Train Epoch: 3 [2560/9840 (26%)]\tLoss: 0.088776\tTime: 57.11\n",
      "Train Epoch: 3 [3840/9840 (39%)]\tLoss: 0.059329\tTime: 84.72\n",
      "Train Epoch: 3 [5120/9840 (52%)]\tLoss: 0.088360\tTime: 112.42\n",
      "Train Epoch: 3 [6400/9840 (65%)]\tLoss: 0.027759\tTime: 140.02\n",
      "Train Epoch: 3 [7680/9840 (78%)]\tLoss: 0.071880\tTime: 167.64\n",
      "Train Epoch: 3 [8960/9840 (92%)]\tLoss: 0.049741\tTime: 195.15\n",
      "Train Epoch: 3 Accuracy: 0/9840 (0.00%)\n",
      "\n",
      "211.689813\n",
      "Train Epoch: 4 [0/9840 (0%)]\tLoss: 0.016810\tTime: 1.36\n",
      "Train Epoch: 4 [1280/9840 (13%)]\tLoss: 0.037543\tTime: 28.85\n",
      "Train Epoch: 4 [2560/9840 (26%)]\tLoss: 0.019678\tTime: 56.19\n",
      "Train Epoch: 4 [3840/9840 (39%)]\tLoss: 0.011689\tTime: 83.51\n",
      "Train Epoch: 4 [5120/9840 (52%)]\tLoss: 0.021162\tTime: 110.80\n",
      "Train Epoch: 4 [6400/9840 (65%)]\tLoss: 0.018969\tTime: 138.06\n",
      "Train Epoch: 4 [7680/9840 (78%)]\tLoss: 0.013596\tTime: 165.27\n",
      "Train Epoch: 4 [8960/9840 (92%)]\tLoss: 0.018397\tTime: 192.31\n",
      "Train Epoch: 4 Accuracy: 0/9840 (0.00%)\n",
      "\n",
      "208.640769\n",
      "Train Epoch: 5 [0/9840 (0%)]\tLoss: 0.007107\tTime: 1.36\n",
      "Train Epoch: 5 [1280/9840 (13%)]\tLoss: 0.009340\tTime: 28.15\n",
      "Train Epoch: 5 [2560/9840 (26%)]\tLoss: 0.007308\tTime: 54.99\n",
      "Train Epoch: 5 [3840/9840 (39%)]\tLoss: 0.006244\tTime: 81.76\n",
      "Train Epoch: 5 [5120/9840 (52%)]\tLoss: 0.006597\tTime: 108.52\n",
      "Train Epoch: 5 [6400/9840 (65%)]\tLoss: 0.014145\tTime: 135.31\n",
      "Train Epoch: 5 [7680/9840 (78%)]\tLoss: 0.005368\tTime: 162.09\n",
      "Train Epoch: 5 [8960/9840 (92%)]\tLoss: 0.008859\tTime: 188.73\n",
      "Train Epoch: 5 Accuracy: 0/9840 (0.00%)\n",
      "\n",
      "204.685634\n",
      "Train Epoch: 6 [0/9840 (0%)]\tLoss: 0.005870\tTime: 1.33\n",
      "Train Epoch: 6 [1280/9840 (13%)]\tLoss: 0.007020\tTime: 27.86\n",
      "Train Epoch: 6 [2560/9840 (26%)]\tLoss: 0.006326\tTime: 54.47\n",
      "Train Epoch: 6 [3840/9840 (39%)]\tLoss: 0.007327\tTime: 80.87\n",
      "Train Epoch: 6 [5120/9840 (52%)]\tLoss: 0.005940\tTime: 107.38\n",
      "Train Epoch: 6 [6400/9840 (65%)]\tLoss: 0.005674\tTime: 133.64\n",
      "Train Epoch: 6 [7680/9840 (78%)]\tLoss: 0.005066\tTime: 160.30\n",
      "Train Epoch: 6 [8960/9840 (92%)]\tLoss: 0.005261\tTime: 186.59\n",
      "Train Epoch: 6 Accuracy: 0/9840 (0.00%)\n",
      "\n",
      "202.384839\n",
      "Train Epoch: 7 [0/9840 (0%)]\tLoss: 0.024124\tTime: 1.43\n",
      "Train Epoch: 7 [1280/9840 (13%)]\tLoss: 0.006840\tTime: 27.75\n",
      "Train Epoch: 7 [2560/9840 (26%)]\tLoss: 0.006564\tTime: 54.07\n",
      "Train Epoch: 7 [3840/9840 (39%)]\tLoss: 0.005399\tTime: 80.33\n",
      "Train Epoch: 7 [5120/9840 (52%)]\tLoss: 0.009939\tTime: 106.65\n",
      "Train Epoch: 7 [6400/9840 (65%)]\tLoss: 0.005694\tTime: 133.07\n",
      "Train Epoch: 7 [7680/9840 (78%)]\tLoss: 0.006455\tTime: 159.32\n",
      "Train Epoch: 7 [8960/9840 (92%)]\tLoss: 0.005653\tTime: 185.68\n",
      "Train Epoch: 7 Accuracy: 0/9840 (0.00%)\n",
      "\n",
      "201.400334\n",
      "Train Epoch: 8 [0/9840 (0%)]\tLoss: 0.005415\tTime: 1.31\n",
      "Train Epoch: 8 [1280/9840 (13%)]\tLoss: 0.005330\tTime: 27.65\n",
      "Train Epoch: 8 [2560/9840 (26%)]\tLoss: 0.005247\tTime: 53.98\n",
      "Train Epoch: 8 [3840/9840 (39%)]\tLoss: 0.005085\tTime: 80.26\n",
      "Train Epoch: 8 [5120/9840 (52%)]\tLoss: 0.005105\tTime: 106.55\n",
      "Train Epoch: 8 [6400/9840 (65%)]\tLoss: 0.005994\tTime: 132.82\n",
      "Train Epoch: 8 [7680/9840 (78%)]\tLoss: 0.008765\tTime: 159.07\n",
      "Train Epoch: 8 [8960/9840 (92%)]\tLoss: 0.016214\tTime: 185.16\n",
      "Train Epoch: 8 Accuracy: 0/9840 (0.00%)\n",
      "\n",
      "200.893889\n",
      "Train Epoch: 9 [0/9840 (0%)]\tLoss: 0.005534\tTime: 1.31\n",
      "Train Epoch: 9 [1280/9840 (13%)]\tLoss: 0.005223\tTime: 27.61\n",
      "Train Epoch: 9 [2560/9840 (26%)]\tLoss: 0.005289\tTime: 53.76\n",
      "Train Epoch: 9 [3840/9840 (39%)]\tLoss: 0.005443\tTime: 80.04\n",
      "Train Epoch: 9 [5120/9840 (52%)]\tLoss: 0.005693\tTime: 106.20\n",
      "Train Epoch: 9 [6400/9840 (65%)]\tLoss: 0.005194\tTime: 132.37\n",
      "Train Epoch: 9 [7680/9840 (78%)]\tLoss: 0.005142\tTime: 158.58\n",
      "Train Epoch: 9 [8960/9840 (92%)]\tLoss: 0.005310\tTime: 184.82\n",
      "Train Epoch: 9 Accuracy: 0/9840 (0.00%)\n",
      "\n",
      "200.65469\n",
      "Train Epoch: 10 [0/9840 (0%)]\tLoss: 0.005404\tTime: 1.35\n",
      "Train Epoch: 10 [1280/9840 (13%)]\tLoss: 0.005113\tTime: 27.38\n",
      "Train Epoch: 10 [2560/9840 (26%)]\tLoss: 0.005064\tTime: 53.62\n",
      "Train Epoch: 10 [3840/9840 (39%)]\tLoss: 0.005403\tTime: 79.71\n",
      "Train Epoch: 10 [5120/9840 (52%)]\tLoss: 0.005115\tTime: 105.82\n",
      "Train Epoch: 10 [6400/9840 (65%)]\tLoss: 0.005930\tTime: 132.06\n",
      "Train Epoch: 10 [7680/9840 (78%)]\tLoss: 0.012097\tTime: 158.13\n",
      "Train Epoch: 10 [8960/9840 (92%)]\tLoss: 0.005878\tTime: 184.40\n",
      "Train Epoch: 10 Accuracy: 0/9840 (0.00%)\n",
      "\n",
      "200.111368\n",
      "Train Epoch: 11 [0/9840 (0%)]\tLoss: 0.005279\tTime: 1.31\n",
      "Train Epoch: 11 [1280/9840 (13%)]\tLoss: 0.005051\tTime: 27.42\n",
      "Train Epoch: 11 [2560/9840 (26%)]\tLoss: 0.005100\tTime: 53.50\n",
      "Train Epoch: 11 [3840/9840 (39%)]\tLoss: 0.006005\tTime: 79.57\n",
      "Train Epoch: 11 [5120/9840 (52%)]\tLoss: 0.005453\tTime: 105.74\n",
      "Train Epoch: 11 [6400/9840 (65%)]\tLoss: 0.005110\tTime: 131.97\n",
      "Train Epoch: 11 [7680/9840 (78%)]\tLoss: 0.005097\tTime: 158.25\n",
      "Train Epoch: 11 [8960/9840 (92%)]\tLoss: 0.005113\tTime: 184.28\n",
      "Train Epoch: 11 Accuracy: 0/9840 (0.00%)\n",
      "\n",
      "199.981533\n",
      "Train Epoch: 12 [0/9840 (0%)]\tLoss: 0.006712\tTime: 1.31\n",
      "Train Epoch: 12 [1280/9840 (13%)]\tLoss: 0.005026\tTime: 27.26\n",
      "Train Epoch: 12 [2560/9840 (26%)]\tLoss: 0.005029\tTime: 53.41\n",
      "Train Epoch: 12 [3840/9840 (39%)]\tLoss: 0.005296\tTime: 79.38\n",
      "Train Epoch: 12 [5120/9840 (52%)]\tLoss: 0.005221\tTime: 105.43\n",
      "Train Epoch: 12 [6400/9840 (65%)]\tLoss: 0.005730\tTime: 131.62\n",
      "Train Epoch: 12 [7680/9840 (78%)]\tLoss: 0.005113\tTime: 157.70\n",
      "Train Epoch: 12 [8960/9840 (92%)]\tLoss: 0.005117\tTime: 183.49\n",
      "Train Epoch: 12 Accuracy: 0/9840 (0.00%)\n",
      "\n",
      "199.226414\n",
      "Train Epoch: 13 [0/9840 (0%)]\tLoss: 0.005389\tTime: 1.32\n",
      "Train Epoch: 13 [1280/9840 (13%)]\tLoss: 0.005077\tTime: 27.40\n",
      "Train Epoch: 13 [2560/9840 (26%)]\tLoss: 0.005089\tTime: 53.47\n",
      "Train Epoch: 13 [3840/9840 (39%)]\tLoss: 0.005010\tTime: 79.32\n",
      "Train Epoch: 13 [5120/9840 (52%)]\tLoss: 0.005507\tTime: 105.44\n",
      "Train Epoch: 13 [6400/9840 (65%)]\tLoss: 0.005095\tTime: 131.40\n",
      "Train Epoch: 13 [7680/9840 (78%)]\tLoss: 0.005245\tTime: 157.39\n",
      "Train Epoch: 13 [8960/9840 (92%)]\tLoss: 0.005051\tTime: 183.42\n",
      "Train Epoch: 13 Accuracy: 0/9840 (0.00%)\n",
      "\n",
      "198.872943\n",
      "Train Epoch: 14 [0/9840 (0%)]\tLoss: 0.005542\tTime: 1.31\n",
      "Train Epoch: 14 [1280/9840 (13%)]\tLoss: 0.005182\tTime: 27.17\n",
      "Train Epoch: 14 [2560/9840 (26%)]\tLoss: 0.005047\tTime: 53.03\n",
      "Train Epoch: 14 [3840/9840 (39%)]\tLoss: 0.005015\tTime: 78.88\n",
      "Train Epoch: 14 [5120/9840 (52%)]\tLoss: 0.005236\tTime: 104.88\n",
      "Train Epoch: 14 [6400/9840 (65%)]\tLoss: 0.005347\tTime: 130.82\n",
      "Train Epoch: 14 [7680/9840 (78%)]\tLoss: 0.005269\tTime: 156.64\n",
      "Train Epoch: 14 [8960/9840 (92%)]\tLoss: 0.005270\tTime: 182.81\n",
      "Train Epoch: 14 Accuracy: 0/9840 (0.00%)\n",
      "\n",
      "198.454821\n",
      "Train Epoch: 15 [0/9840 (0%)]\tLoss: 0.005284\tTime: 1.30\n",
      "Train Epoch: 15 [1280/9840 (13%)]\tLoss: 0.005205\tTime: 27.23\n",
      "Train Epoch: 15 [2560/9840 (26%)]\tLoss: 0.005013\tTime: 53.15\n",
      "Train Epoch: 15 [3840/9840 (39%)]\tLoss: 0.005027\tTime: 78.91\n",
      "Train Epoch: 15 [5120/9840 (52%)]\tLoss: 0.005996\tTime: 104.85\n",
      "Train Epoch: 15 [6400/9840 (65%)]\tLoss: 0.005091\tTime: 130.57\n",
      "Train Epoch: 15 [7680/9840 (78%)]\tLoss: 0.005004\tTime: 156.33\n",
      "Train Epoch: 15 [8960/9840 (92%)]\tLoss: 0.005263\tTime: 182.18\n",
      "Train Epoch: 15 Accuracy: 0/9840 (0.00%)\n",
      "\n",
      "197.610137\n",
      "Train Epoch: 16 [0/9840 (0%)]\tLoss: 0.005052\tTime: 1.30\n",
      "Train Epoch: 16 [1280/9840 (13%)]\tLoss: 0.005084\tTime: 27.39\n",
      "Train Epoch: 16 [2560/9840 (26%)]\tLoss: 0.005068\tTime: 53.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 16 [3840/9840 (39%)]\tLoss: 0.005503\tTime: 78.91\n",
      "Train Epoch: 16 [5120/9840 (52%)]\tLoss: 0.005084\tTime: 104.81\n",
      "Train Epoch: 16 [6400/9840 (65%)]\tLoss: 0.005029\tTime: 130.71\n",
      "Train Epoch: 16 [7680/9840 (78%)]\tLoss: 0.005028\tTime: 156.46\n",
      "Train Epoch: 16 [8960/9840 (92%)]\tLoss: 0.005011\tTime: 182.19\n",
      "Train Epoch: 16 Accuracy: 0/9840 (0.00%)\n",
      "\n",
      "197.785557\n",
      "Train Epoch: 17 [0/9840 (0%)]\tLoss: 0.005036\tTime: 1.30\n",
      "Train Epoch: 17 [1280/9840 (13%)]\tLoss: 0.005033\tTime: 27.26\n",
      "Train Epoch: 17 [2560/9840 (26%)]\tLoss: 0.005099\tTime: 52.95\n",
      "Train Epoch: 17 [3840/9840 (39%)]\tLoss: 0.005145\tTime: 78.55\n",
      "Train Epoch: 17 [5120/9840 (52%)]\tLoss: 0.005057\tTime: 104.27\n",
      "Train Epoch: 17 [6400/9840 (65%)]\tLoss: 0.005088\tTime: 130.09\n",
      "Train Epoch: 17 [7680/9840 (78%)]\tLoss: 0.005062\tTime: 155.86\n",
      "Train Epoch: 17 [8960/9840 (92%)]\tLoss: 0.006292\tTime: 181.85\n",
      "Train Epoch: 17 Accuracy: 0/9840 (0.00%)\n",
      "\n",
      "197.414508\n",
      "Train Epoch: 18 [0/9840 (0%)]\tLoss: 0.005069\tTime: 1.30\n",
      "Train Epoch: 18 [1280/9840 (13%)]\tLoss: 0.005028\tTime: 26.92\n",
      "Train Epoch: 18 [2560/9840 (26%)]\tLoss: 0.008642\tTime: 52.66\n",
      "Train Epoch: 18 [3840/9840 (39%)]\tLoss: 0.005067\tTime: 78.63\n",
      "Train Epoch: 18 [5120/9840 (52%)]\tLoss: 0.006691\tTime: 104.41\n",
      "Train Epoch: 18 [6400/9840 (65%)]\tLoss: 0.005048\tTime: 130.07\n",
      "Train Epoch: 18 [7680/9840 (78%)]\tLoss: 0.005009\tTime: 155.90\n",
      "Train Epoch: 18 [8960/9840 (92%)]\tLoss: 0.005021\tTime: 181.66\n",
      "Train Epoch: 18 Accuracy: 0/9840 (0.00%)\n",
      "\n",
      "197.064787\n",
      "Train Epoch: 19 [0/9840 (0%)]\tLoss: 0.005125\tTime: 1.30\n",
      "Train Epoch: 19 [1280/9840 (13%)]\tLoss: 0.005019\tTime: 26.82\n",
      "Train Epoch: 19 [2560/9840 (26%)]\tLoss: 0.005667\tTime: 52.56\n",
      "Train Epoch: 19 [3840/9840 (39%)]\tLoss: 0.005000\tTime: 78.53\n",
      "Train Epoch: 19 [5120/9840 (52%)]\tLoss: 0.005011\tTime: 104.21\n",
      "Train Epoch: 19 [6400/9840 (65%)]\tLoss: 0.005345\tTime: 129.72\n",
      "Train Epoch: 19 [7680/9840 (78%)]\tLoss: 0.005004\tTime: 155.34\n",
      "Train Epoch: 19 [8960/9840 (92%)]\tLoss: 0.005003\tTime: 180.96\n",
      "Train Epoch: 19 Accuracy: 0/9840 (0.00%)\n",
      "\n",
      "196.33235\n",
      "Train Epoch: 20 [0/9840 (0%)]\tLoss: 0.005076\tTime: 1.31\n",
      "Train Epoch: 20 [1280/9840 (13%)]\tLoss: 0.005446\tTime: 27.20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ce900769195e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_acc_this\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_eval_icp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'on'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-55bfcb999760>\u001b[0m in \u001b[0;36mtrain_eval_icp\u001b[0;34m(model, device, train_loader, test_loader, optimizer, criterion, epoch, eval_mode)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mt_ab_pert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_ab_pert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_pert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr_ab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_ab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr_ab_pert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_ab_pert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdist_pert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-6cdd73612b44>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2, R, T, R_pert, T_pert)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mpert_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpert_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpert_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpert_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mpert_emb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpert_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mpert_emb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mpert_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpert_emb1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpert_emb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mpert_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpert_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-d53f677e2804>\u001b[0m in \u001b[0;36mtile\u001b[0;34m(a, dim, n_tile)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrepeat_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_tile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0morder_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minit_dim\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_tile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_time= []\n",
    "train_loss = []\n",
    "train_losses.append(train_loss)\n",
    "tests = []\n",
    "test_acc = []\n",
    "test_results_exp.append(tests)\n",
    "test_accs.append(test_acc)\n",
    "ratio = 0\n",
    "for epoch in range(1,600):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    t1 = datetime.now()\n",
    "    train_error,test_error,test_acc_this = train_eval_icp(model,device, train_loader, test_loader, optimizer,criterion,epoch,'on')\n",
    "    train_loss.extend(train_error)\n",
    "    tests.extend(test_error)\n",
    "    test_acc.extend(test_acc_this)\n",
    "    train_time.append((datetime.now()-t1).total_seconds())\n",
    "    print((datetime.now()-t1).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.cpu(),\"pert_model.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
