{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from data_with_pert import ModelNet40_pert,download,load_data\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from geomloss import SamplesLoss\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import warnings\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torch import jit\n",
    "%matplotlib inline\n",
    "import os\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_results_exp = []\n",
    "test_accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 2 every 30 epochs\"\"\"\n",
    "    lrt = lr * (0.5 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile(a, dim, n_tile):\n",
    "    init_dim = a.size(dim)\n",
    "    repeat_idx = [1] * a.dim()\n",
    "    repeat_idx[dim] = n_tile\n",
    "    a = a.repeat(*(repeat_idx))\n",
    "    order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)])).to(device)\n",
    "    return torch.index_select(a, dim, order_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gwu/ConvexICP/data_with_pert.py:36: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  f = h5py.File(h5_name)\n"
     ]
    }
   ],
   "source": [
    "M=10\n",
    "batch_size= 8\n",
    "data = ModelNet40_pert(num_points=1000, partition='train', gaussian_noise=False,\n",
    "                       unseen=False, factor=4,n_pert=M)\n",
    "data_test = ModelNet40_pert(num_points=1000, partition='test', gaussian_noise=False,\n",
    "                       unseen=False, factor=4,n_pert=M)\n",
    "train_loader = DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(data_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "rl_loader = DataLoader(data_test, batch_size=1, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_icp(model,device, train_loader,test_loader, optimizer,criterion, epoch, eval_mode='on'):\n",
    "    global best\n",
    "    model.train()\n",
    "    final_cal = nn.Sigmoid()\n",
    "    train_error_logs = []\n",
    "    t1 = datetime.now()\n",
    "    test_loss_log = []\n",
    "    test_acc_log = []\n",
    "    train_corrects = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    for batch_idx, (src, target, r_ab, t_ab, _,_,_,_,r_ab_pert,t_ab_pert,lambdas) in enumerate(train_loader):\n",
    "        src = src.to(device)\n",
    "        target = target.to(device)\n",
    "        r_ab = r_ab.to(device)\n",
    "        t_ab = t_ab.to(device)\n",
    "        r_ab_pert = r_ab_pert.to(device)\n",
    "        t_ab_pert = t_ab_pert.to(device)\n",
    "        lambdas = lambdas.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        dist, dist_pert, dist_pert_3,dist_pert_comb = model(src,target,r_ab,t_ab,r_ab_pert,t_ab_pert,lambdas)\n",
    "        loss = criterion(dist,dist_pert, dist_pert_3, dist_pert_comb, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #pred_train = output_train.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "        #train_corrects += pred_train.eq(target.view_as(pred_train)).sum().item()\n",
    "        train_error_logs.append(loss.item())\n",
    "        \n",
    "        if (batch_idx+0) % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime: {:.2f}'.format(\n",
    "                epoch, (batch_idx+0)* len(target), len(train_loader.dataset),\n",
    "                100. * (batch_idx+0) / len(train_loader), loss,(datetime.now()-t1).total_seconds()))\n",
    "\n",
    "        \n",
    "    print('Train Epoch: {} Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "                epoch, train_corrects, len(train_loader.dataset),\n",
    "                100. * train_corrects / len(train_loader.dataset)))\n",
    "    '''\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_test = 0\n",
    "    with torch.no_grad():\n",
    "        for src, target, rotation_ab, translation_ab, rotation_ba, translation_ba, euler_ab, euler_ba in test_loader:\n",
    "            src = src.to(device)\n",
    "            target = target.to(device)\n",
    "            rotation_ab = rotation_ab.to(device)\n",
    "            translation_ab = translation_ab.to(device)\n",
    "            rotation_ba = rotation_ba.to(device)\n",
    "            translation_ba = translation_ba.to(device)\n",
    "            output_test = model(src,target,rotation_ab,translation_ab)\n",
    "            test_loss += criterion(output_test, target).item()*len(src)  # sum up batch loss\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_loss_log.append(test_loss)\n",
    "    print('Test set: Average loss: {:.8f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct_test, len(test_loader.dataset),\n",
    "            100. * correct_test / len(test_loader.dataset)))\n",
    "    '''\n",
    "    return train_error_logs,test_loss_log,test_acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_rt_loss(ra,ta,rb,tb):\n",
    "    r_loss = F.mse_loss(ra,rb).item()\n",
    "    t_loss = F.mse_loss(ta,tb).item()\n",
    "    return r_loss, t_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet(nn.Module):\n",
    "    def __init__(self, emb_dims=512):\n",
    "        super(PointNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(3, 64, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n",
    "        self.conv3 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n",
    "        self.conv4 = nn.Conv1d(64, 128, kernel_size=1, bias=False)\n",
    "        self.conv5 = nn.Conv1d(128, emb_dims, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.bn5 = nn.BatchNorm1d(emb_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPNet(nn.Module):\n",
    "    def __init__(self, emb_dims=512,split = 10):\n",
    "        super(CPNet, self).__init__()\n",
    "        self.split = split\n",
    "        self.base_net = PointNet()\n",
    "        self.loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.05)\n",
    "\n",
    "        \n",
    "    def forward(self, x1, x2, R, T, R_pert, T_pert,lambdas):\n",
    "        \n",
    "        trans = torch.matmul(R, x1) + T.unsqueeze(2)\n",
    "        emb1 = self.base_net(trans)\n",
    "        emb2 = self.base_net(x2)\n",
    "        \n",
    "        dist = self.loss(emb1,emb2)#+self.loss(emb2,emb1)\n",
    "        \n",
    "        #constraint 1\n",
    "        pert_out = torch.matmul(R_pert.permute(1,0,2,3), x1) + T_pert.permute(1,0,2).unsqueeze(3)\n",
    "        pert_out = pert_out.reshape(-1, pert_out.shape[-2],pert_out.shape[-1])\n",
    "        pert_emb1 = self.base_net(pert_out)\n",
    "        pert_emb2 = tile(emb2,0,self.split)\n",
    "        pert_dist = self.loss(pert_emb1,pert_emb2)/self.split #+ self.loss(pert_emb2,pert_emb1)/self.split\n",
    "        pert_dist_1 = pert_dist.reshape(self.split, emb2.shape[0])\n",
    "        \n",
    "        #constraint 2\n",
    "        \n",
    "        lambdas_t = lambdas.unsqueeze(-1).expand(-1,-1,3)\n",
    "        lambdas_r = lambdas_t.unsqueeze(-1).expand(-1,-1,-1,3)\n",
    "        R_pert_3 = lambdas_r*R_pert+(1-lambdas_r)*tile(R,0,self.split).reshape(*R_pert.shape)\n",
    "        T_pert_3 = lambdas_t*T_pert+(1-lambdas_t)*tile(T,0,self.split).reshape(*T_pert.shape)\n",
    "        \n",
    "        pert_out_3 = torch.matmul(R_pert_3.permute(1,0,2,3), x1) + T_pert_3.permute(1,0,2).unsqueeze(3)\n",
    "        pert_out_3 = pert_out_3.reshape(-1, pert_out_3.shape[-2],pert_out_3.shape[-1])\n",
    "        pert_emb1_3 = self.base_net(pert_out_3)\n",
    "        pert_dist_3 = self.loss(pert_emb1_3,pert_emb2)/self.split #+ self.loss(pert_emb2,pert_emb1_3)/self.split\n",
    "        pert_dist_3 = pert_dist_3.reshape(self.split, emb2.shape[0])\n",
    "        \n",
    "        pert_dist_comb = lambdas.T*pert_dist_1+(1-lambdas.T)*dist\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        return dist,pert_dist_1.sum(0),pert_dist_3.sum(0),pert_dist_comb.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_loss(dist, pert_dist_1,pert_dist_3,pert_dist_comb, target, alpha = 1000, beta = 1000, eps = 0.005, eta = 0.005):\n",
    "    loss = torch.mean(dist)\n",
    "    \n",
    "    diff = dist-pert_dist_1\n",
    "    dummy = torch.zeros_like(diff)\n",
    "    pert_loss = F.hinge_embedding_loss(diff,dummy,margin = eps)\n",
    "    \n",
    "    diff_2 = pert_dist_3-pert_dist_comb\n",
    "    dummy_2 = torch.zeros_like(diff_2)\n",
    "    pert_loss_2 = F.hinge_embedding_loss(diff_2,dummy_2,margin = eta)\n",
    "    \n",
    "    return loss+beta*pert_loss_2\n",
    "#+alpha*pert_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CPNet(split=M).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = combine_loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "best=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/9840 (0%)]\tLoss: 296.131042\tTime: 0.42\n",
      "Train Epoch: 1 [160/9840 (2%)]\tLoss: 32.612011\tTime: 8.14\n",
      "Train Epoch: 1 [320/9840 (3%)]\tLoss: 37.148277\tTime: 15.52\n",
      "Train Epoch: 1 [480/9840 (5%)]\tLoss: 54.193703\tTime: 23.33\n",
      "Train Epoch: 1 [640/9840 (7%)]\tLoss: 23.446484\tTime: 30.99\n",
      "Train Epoch: 1 [800/9840 (8%)]\tLoss: 27.590219\tTime: 38.56\n",
      "Train Epoch: 1 [960/9840 (10%)]\tLoss: 15.318518\tTime: 46.22\n",
      "Train Epoch: 1 [1120/9840 (11%)]\tLoss: 8.432614\tTime: 53.68\n",
      "Train Epoch: 1 [1280/9840 (13%)]\tLoss: 6.981595\tTime: 61.07\n",
      "Train Epoch: 1 [1440/9840 (15%)]\tLoss: 5.900253\tTime: 68.25\n",
      "Train Epoch: 1 [1600/9840 (16%)]\tLoss: 5.772002\tTime: 75.38\n",
      "Train Epoch: 1 [1760/9840 (18%)]\tLoss: 5.305846\tTime: 82.50\n",
      "Train Epoch: 1 [1920/9840 (20%)]\tLoss: 5.371316\tTime: 89.63\n",
      "Train Epoch: 1 [2080/9840 (21%)]\tLoss: 5.204899\tTime: 96.75\n",
      "Train Epoch: 1 [2240/9840 (23%)]\tLoss: 5.317038\tTime: 103.81\n",
      "Train Epoch: 1 [2400/9840 (24%)]\tLoss: 5.076724\tTime: 110.74\n",
      "Train Epoch: 1 [2560/9840 (26%)]\tLoss: 5.075683\tTime: 117.62\n",
      "Train Epoch: 1 [2720/9840 (28%)]\tLoss: 5.108438\tTime: 124.48\n",
      "Train Epoch: 1 [2880/9840 (29%)]\tLoss: 5.184674\tTime: 131.33\n",
      "Train Epoch: 1 [3040/9840 (31%)]\tLoss: 5.071157\tTime: 138.19\n",
      "Train Epoch: 1 [3200/9840 (33%)]\tLoss: 5.059193\tTime: 145.05\n",
      "Train Epoch: 1 [3360/9840 (34%)]\tLoss: 5.025688\tTime: 151.90\n",
      "Train Epoch: 1 [3520/9840 (36%)]\tLoss: 5.019655\tTime: 158.76\n",
      "Train Epoch: 1 [3680/9840 (37%)]\tLoss: 5.045837\tTime: 165.62\n",
      "Train Epoch: 1 [3840/9840 (39%)]\tLoss: 5.026129\tTime: 172.47\n",
      "Train Epoch: 1 [4000/9840 (41%)]\tLoss: 5.018609\tTime: 179.30\n",
      "Train Epoch: 1 [4160/9840 (42%)]\tLoss: 5.013497\tTime: 186.09\n",
      "Train Epoch: 1 [4320/9840 (44%)]\tLoss: 5.019991\tTime: 192.79\n",
      "Train Epoch: 1 [4480/9840 (46%)]\tLoss: 5.015105\tTime: 199.45\n",
      "Train Epoch: 1 [4640/9840 (47%)]\tLoss: 5.009252\tTime: 206.10\n",
      "Train Epoch: 1 [4800/9840 (49%)]\tLoss: 5.051718\tTime: 212.72\n",
      "Train Epoch: 1 [4960/9840 (50%)]\tLoss: 5.005316\tTime: 219.32\n",
      "Train Epoch: 1 [5120/9840 (52%)]\tLoss: 5.006578\tTime: 225.91\n",
      "Train Epoch: 1 [5280/9840 (54%)]\tLoss: 5.025755\tTime: 232.50\n",
      "Train Epoch: 1 [5440/9840 (55%)]\tLoss: 5.044914\tTime: 239.09\n",
      "Train Epoch: 1 [5600/9840 (57%)]\tLoss: 5.002580\tTime: 245.66\n",
      "Train Epoch: 1 [5760/9840 (59%)]\tLoss: 5.005187\tTime: 252.23\n",
      "Train Epoch: 1 [5920/9840 (60%)]\tLoss: 5.010566\tTime: 258.73\n",
      "Train Epoch: 1 [6080/9840 (62%)]\tLoss: 5.005758\tTime: 265.23\n",
      "Train Epoch: 1 [6240/9840 (63%)]\tLoss: 5.002844\tTime: 271.70\n",
      "Train Epoch: 1 [6400/9840 (65%)]\tLoss: 5.003827\tTime: 278.17\n",
      "Train Epoch: 1 [6560/9840 (67%)]\tLoss: 5.003478\tTime: 284.55\n",
      "Train Epoch: 1 [6720/9840 (68%)]\tLoss: 5.001406\tTime: 290.99\n",
      "Train Epoch: 1 [6880/9840 (70%)]\tLoss: 5.000581\tTime: 297.39\n",
      "Train Epoch: 1 [7040/9840 (72%)]\tLoss: 5.010109\tTime: 303.76\n",
      "Train Epoch: 1 [7200/9840 (73%)]\tLoss: 5.007123\tTime: 310.16\n",
      "Train Epoch: 1 [7360/9840 (75%)]\tLoss: 5.001189\tTime: 316.52\n",
      "Train Epoch: 1 [7520/9840 (76%)]\tLoss: 5.002755\tTime: 322.92\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-ce900769195e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_acc_this\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_eval_icp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'on'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-bf0dafd57aec>\u001b[0m in \u001b[0;36mtrain_eval_icp\u001b[0;34m(model, device, train_loader, test_loader, optimizer, criterion, epoch, eval_mode)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#pred_train = output_train.argmax(dim=1, keepdim=True) # get the index of the max log-probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#train_corrects += pred_train.eq(target.view_as(pred_train)).sum().item()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mtrain_error_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_time= []\n",
    "train_loss = []\n",
    "train_losses.append(train_loss)\n",
    "tests = []\n",
    "test_acc = []\n",
    "test_results_exp.append(tests)\n",
    "test_accs.append(test_acc)\n",
    "ratio = 0\n",
    "for epoch in range(1,600):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    t1 = datetime.now()\n",
    "    train_error,test_error,test_acc_this = train_eval_icp(model,device, train_loader, test_loader, optimizer,criterion,epoch,'on')\n",
    "    train_loss.extend(train_error)\n",
    "    tests.extend(test_error)\n",
    "    test_acc.extend(test_acc_this)\n",
    "    train_time.append((datetime.now()-t1).total_seconds())\n",
    "    print((datetime.now()-t1).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.cpu(),\"pert_model_new.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (src, target, r_ab, t_ab, _,_,_,_,r_ab_pert,t_ab_pert,r_ab_pert_2,t_ab_pert_2,lambdas) in enumerate(train_loader):\n",
    "    print(lambdas.shape,r_ab_pert.shape)\n",
    "    src = src.to(device)\n",
    "    target = target.to(device)\n",
    "    r_ab = r_ab.to(device)\n",
    "    t_ab = t_ab.to(device)\n",
    "    r_ab_pert = r_ab_pert.to(device)\n",
    "    t_ab_pert = t_ab_pert.to(device)\n",
    "    r_ab_pert_2 = r_ab_pert_2.to(device)\n",
    "    t_ab_pert_2 = t_ab_pert_2.to(device)\n",
    "    lambdas = lambdas.to(device)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
